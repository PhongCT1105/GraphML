{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CS 4099: ST: Graph Machine Learning**\n",
    "\n",
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel $\\rightarrow$ Restart) and then **run all cells** (in the menubar, select Cell $\\rightarrow$ Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Phong Cao\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJaKxlgoflZv"
   },
   "source": [
    "# CS4099: Colab 1\n",
    "\n",
    "<font color='fucsia'>DUE: 11:59pm ET Thursday January 30</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fg8EuS3mdf-Z"
   },
   "source": [
    "# 1. Linear Regression (4 questions + 2 ISU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otHPP4utf2hz"
   },
   "source": [
    "\n",
    "\n",
    "A (multiple) linear regression is expressed as:\n",
    "\n",
    "$y = w_0 + w_1 x_1 + \\ldots + w_m x_m$\n",
    "\n",
    "When exploring the California Housing Dataset in Colab 0, we saw that these 4 features seem to be informative about the target variable:\n",
    "- MedInc\n",
    "- Population\n",
    "- Latitude\n",
    "- Longitude\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "TAmH_SR9rMpp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Load the California Housing dataset from sklearn.\n",
    "housing = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-bMvaHRil4PU"
   },
   "source": [
    "Q1. Which of these 4 features should be included **in a linear regression model**? Define variable `col_names` based on your answer.\n",
    "\n",
    "TIP: Think about which sign you expect each coefficient $w_j$ to have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "id": "dwEXCa6ylLFj",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7c32b4dc7b00684a28e8671ff3198c4e",
     "grade": true,
     "grade_id": "cell-53d2c701d1c46b59",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MedInc', 'Population', 'Latitude', 'Longitude']\n"
     ]
    }
   ],
   "source": [
    "# A1. (1 line)\n",
    "# YOUR CODE HERE\n",
    "col_names = ['MedInc', 'Population', 'Latitude', 'Longitude']\n",
    "\n",
    "print(col_names)\n",
    "\n",
    "# get indices based on col_names\n",
    "cols = [housing.feature_names.index(col_name) for col_name in col_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dM_DFnV3ow2N"
   },
   "source": [
    "We will split data into training and test using only selected features.\n",
    "\n",
    "ML train-test splits are typically 70-30 and 80-20. However, in this case, we will be more aggressive and set it to 90-10, because the training set is very large relative to the number of model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "ZhBDoZaEqMl9"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "PRO TIP: We must split the data into training and test sets before we\n",
    "start instantiating models. If you wait to this later, there is a higher chance\n",
    "that you will make a mistake that leads to leakage between training & test sets.\n",
    "\n",
    "PRO TIP: Splitting the data involves randomization. In order to ensure\n",
    "reproducibility, set the random seed before/when you calling functions.\n",
    "'''\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(housing.data[:,cols], housing.target, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vb4sPBgqyUw"
   },
   "source": [
    "Q2. Now you need to z-normalize the features, i.e., subtract their respective means and divide by their respective standard deviations. Make sure to apply same transformations to validation and test sets too.\n",
    "\n",
    "You can either do it manually or use a function from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "id": "AC2wXxiSriD9",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d79dc98febdaec907b6f99aa1036338",
     "grade": true,
     "grade_id": "cell-d424d479757139cc",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01883674 -0.00969912 -0.80886286  0.64130268]\n",
      " [ 2.18523706 -0.13251859  0.9803686  -1.43975779]\n",
      " [-0.10658316  1.09743063  0.8539046  -1.28505066]\n",
      " [-0.37769845 -0.55712303  1.46280536 -0.81593871]\n",
      " [ 0.55838671 -0.01408553  0.55882192 -0.08731802]] (18576, 4)\n",
      "[[-1.15848302 -0.03075389  0.19816532  0.28198288]\n",
      " [-0.7091262   0.12101588 -0.23274907  0.05740801]\n",
      " [-0.20759726 -0.102691    1.01315557 -1.42977669]\n",
      " [ 0.9857012   0.24383535 -0.63556034  0.42670891]\n",
      " [-0.07814486 -0.31937963  0.46046103 -1.1752585 ]] (2064, 4)\n"
     ]
    }
   ],
   "source": [
    "# A2. (~3-4 lines)\n",
    "# YOUR CODE HERE\n",
    "\n",
    "means = np.mean(X_train, axis = 0)\n",
    "stds = np.std(X_train, axis=0)\n",
    "\n",
    "X_train = (X_train - means) / stds\n",
    "X_test = (X_test - means) / stds\n",
    "\n",
    "print(X_train[:5], X_train.shape)\n",
    "print(X_test[:5], X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQore2EGtEfI"
   },
   "source": [
    "## Loss function and Exact Solution\n",
    "\n",
    "Here we'll use the MSE as the loss function. This leads to an optimization problem known as the Ordinary Least Squares (OLS):\n",
    "$$\n",
    "\\min_\\mathbf{w} \\|\\mathbf{X}\\mathbf{w} - \\mathbf{y}\\|^2,\n",
    "$$\n",
    "where $\\mathbf{X} \\in \\mathbb{R}^{n \\times (1+m)}$ is the design matrix where each row represents one observation and the first column is a vector of 1's.\n",
    "\n",
    "\n",
    "The exact solution to OLS is given by\n",
    "$$\\mathbf{w} = \\left(\\sum_{i=1}^n \\mathbf{x}^{(i)} {\\mathbf{x}^{(i)}}^\\top \\right)^{-1}\\sum_{i=1}^n \\mathbf{x}^{(i)} \\mathbf{y}^{(i)}. $$\n",
    "\n",
    "PRO TIP: Inside the parentheses is a sum of outer products. The result IS NOT A SCALAR. As a sanity check, make sure that the matrix dimensions are $(m+1)\\times(m+1)$.\n",
    "\n",
    "The solution can be rewritten as\n",
    "$$\\mathbf{w} = \\left( \\mathbf{X}^\\top \\mathbf{X} \\right)^{-1}\\mathbf{X}^\\top \\mathbf{y}.$$\n",
    "\n",
    "To think about: Is $\\mathbf{X}^\\top \\mathbf{X}$ always invertible?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jv7Qn4WntYKO"
   },
   "source": [
    "Q3. We will find the optmimal parameters for the Linear Regression by computing the expression above for $w$. Make sure to augment matrix $\\mathbf{X}$ (training and test) so that the intercept is automatically included in $w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "2kFb4kjwtXiK",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "715a5e701c9fe98c3b345d30f954a3ed",
     "grade": true,
     "grade_id": "cell-28307497752a461b",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "6def794e-942a-49f4-919d-b2220e437c52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal parameters (w): [ 2.06861739  0.69823853 -0.04568336 -1.03988835 -0.99906966]\n"
     ]
    }
   ],
   "source": [
    "# Add a column of ones to X_train and X_test for the intercept\n",
    "X_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
    "X_test = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n",
    "\n",
    "# Calculate the inverse of X^T * X\n",
    "X_invert = np.linalg.inv(X_train.T @ X_train)\n",
    "\n",
    "# Calculate the weights w using the correct OLS formula\n",
    "w = X_invert @ X_train.T @ y_train\n",
    "\n",
    "print(\"Optimal parameters (w):\", w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ik3LRjRduf13"
   },
   "source": [
    "## Prediction and Evaluation\n",
    "\n",
    "Q4. (a) Now use model parameters $w$ to predict house prices for the test set.\n",
    "\n",
    "(b) Also, compute the root mean squared error. Is this high or low relative to the values of $y$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "id": "6LnIeYyfhA7Z",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b03e434005911f5a258da68616c1aa12",
     "grade": true,
     "grade_id": "cell-7c248ca3c7e9af87",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7733345  1.75262819 2.30323408 ... 3.06832575 1.63521217 1.75067573]\n"
     ]
    }
   ],
   "source": [
    "# A4a. (1 line)\n",
    "# YOUR CODE HERE\n",
    "y_test_pred = X_test @ w\n",
    "print(y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "4T-DANOyuiRq",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f7a3d6cc57067c5047b458e53f6d0641",
     "grade": true,
     "grade_id": "cell-006a8a73b08eff0d",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "a608452f-214b-4673-9f42-4b0f9ade3cc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE 0.7609 is low relative to the values of y\n"
     ]
    }
   ],
   "source": [
    "# A4b. (~1-2 lines + CONDITION)\n",
    "# YOUR CODE HERE\n",
    "# A4b. (~1-2 lines + CONDITION)\n",
    "# Compute RMSE\n",
    "rmse = np.sqrt(np.mean((y_test - y_test_pred) ** 2))\n",
    "\n",
    "# Compare RMSE to the range of y\n",
    "if rmse < 0.2 * (np.max(y_test) - np.min(y_test)):  # If RMSE is less than 20% of the range\n",
    "    print(f'RMSE {rmse:.4f} is low relative to the values of y')\n",
    "else:\n",
    "    print(f'RMSE {rmse:.4f} is high relative to the values of y')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5I0L2TroCj4"
   },
   "source": [
    "Q5. [ISU STUDENTS ONLY] Compute the Mean Absolute Error on the test set, defined as\n",
    "$\\textrm{MAE} = \\frac{1}{n}\\sum_{i=1}^n |y^{(i)} - \\widehat{y}^{(i)}|$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "90pbsHngoBag",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2c4c5a07489e924496e419d8485f6c93",
     "grade": true,
     "grade_id": "cell-8df6b9d27d056d15",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# # A5. (~1-2 lines)\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAi6zUNybVnE"
   },
   "source": [
    "Let's plot the ground-truth prices $y$ against the predictions $\\hat y$. We will add the line y=x to facilitate the visual analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "id": "6XhL_yu-bUzU",
    "outputId": "f7a52548-47f0-4dab-be48-3b44307a67a2"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot y and y_pred\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4,3.5))\n",
    "ax.scatter(y_test, y_pred, alpha=.1)\n",
    "ax.set_xlabel('ground-truth y')\n",
    "ax.set_ylabel(r'prediction $\\hat y$')\n",
    "ax.set_ylim(ax.get_xlim())\n",
    "ax.plot(ax.get_xlim(), ax.get_xlim(), 'k--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kn7i96JPcdiE"
   },
   "source": [
    "Q6. [ISU STUDENTS ONLY] Based on the plot above, can we conclude that this linear regression model tends to overestimate the ground-truth values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "id": "iZr3vPiycs8r",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ebbb8f106c750a07f5bfa02a1775851c",
     "grade": true,
     "grade_id": "cell-1fe661cc6c329507",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrv6UfeA9eFZ"
   },
   "source": [
    "# 2. Gradient Descent (3 questions + 1 ISU)\n",
    "\n",
    "You will now apply the gradient descent to the linear regression using the California Housing dataset to see if you can get to the same coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpUwwfL993o4"
   },
   "source": [
    "First, you need to initialize the parameter vector w. A good way to do so is to use the standard Gaussian distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P7d7mqD3-byt"
   },
   "source": [
    "Q1. (a) Initialize vector `w0` from a Gaussian distribution with sdev 1e-2.\n",
    "\n",
    "(b) Compute the predictions `y_pred` for the training set using `w0`.\n",
    "\n",
    "(c) Compute the MSE loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "x1u9B-uq-Sst",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3ab1af8802c0999919c780e581eb55bc",
     "grade": true,
     "grade_id": "cell-d77dfc4c13877613",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "178968b6-45ea-4fe7-bf14-4d6ce64b1df3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model coefficients: [ 0.005 -0.001  0.006  0.015 -0.002]\n",
      "[-0.00889049  0.01938998  0.02823665  0.02607045  0.01281934]\n",
      "Training loss = 5.598\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "# A1a. (~2 lines)\n",
    "# YOUR CODE HERE\n",
    "w0 = np.random.randn(X_train.shape[1]) * 1e-2\n",
    "print(f'Model coefficients: {np.round(w0,3)}')\n",
    "\n",
    "# A1b. (~1 line)\n",
    "# YOUR CODE HERE\n",
    "y_pred = X_train @ w0\n",
    "print(y_pred[:5])\n",
    "\n",
    "# A1c. (~1-2 lines)\n",
    "# PRO TIP: tracking the train loss at the beginning and during optimization is\n",
    "# key for debugging, determining convergence\n",
    "# YOUR CODE HERE\n",
    "loss = np.mean((y_train - y_pred) ** 2)  # Compute the mean squared error\n",
    "print(f'Training loss = {loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8rPcU3t_GZL"
   },
   "source": [
    "Q2. Implement the function `gradient_descent` below and run it for 10 steps using a learning rate $\\alpha = 0.2$. Print the loss at each iteration, before performing a step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss = 5.598\n",
      "Training loss = 2.443\n",
      "Training loss = 1.306\n",
      "Training loss = 0.893\n",
      "Training loss = 0.740\n",
      "Training loss = 0.681\n",
      "Training loss = 0.657\n",
      "Training loss = 0.644\n",
      "Training loss = 0.637\n",
      "Training loss = 0.631\n",
      "Training loss = 0.626\n",
      "Training loss = 0.622\n",
      "Training loss = 0.618\n",
      "Training loss = 0.614\n",
      "Training loss = 0.610\n",
      "Training loss = 0.607\n",
      "Training loss = 0.604\n",
      "Training loss = 0.601\n",
      "Training loss = 0.598\n",
      "Training loss = 0.595\n",
      "Training loss = 0.592\n",
      "Training loss = 0.590\n",
      "Training loss = 0.588\n",
      "Training loss = 0.585\n",
      "Training loss = 0.583\n",
      "Training loss = 0.581\n",
      "Training loss = 0.580\n",
      "Training loss = 0.578\n",
      "Training loss = 0.576\n",
      "Training loss = 0.575\n",
      "Training loss = 0.573\n",
      "Training loss = 0.572\n",
      "Training loss = 0.571\n",
      "Training loss = 0.569\n",
      "Training loss = 0.568\n",
      "Training loss = 0.567\n",
      "Training loss = 0.566\n",
      "Training loss = 0.565\n",
      "Training loss = 0.564\n",
      "Training loss = 0.563\n",
      "Training loss = 0.563\n",
      "Training loss = 0.562\n",
      "Training loss = 0.561\n",
      "Training loss = 0.560\n",
      "Training loss = 0.560\n",
      "Training loss = 0.559\n",
      "Training loss = 0.558\n",
      "Training loss = 0.558\n",
      "Training loss = 0.557\n",
      "Training loss = 0.557\n",
      "Training loss = 0.556\n",
      "Training loss = 0.556\n",
      "Training loss = 0.556\n",
      "Training loss = 0.555\n",
      "Training loss = 0.555\n",
      "Training loss = 0.554\n",
      "Training loss = 0.554\n",
      "Training loss = 0.554\n",
      "Training loss = 0.554\n",
      "Training loss = 0.553\n",
      "Training loss = 0.553\n",
      "Training loss = 0.553\n",
      "Training loss = 0.553\n",
      "Training loss = 0.552\n",
      "Training loss = 0.552\n",
      "Training loss = 0.552\n",
      "Training loss = 0.552\n",
      "Training loss = 0.552\n",
      "Training loss = 0.551\n",
      "Training loss = 0.551\n",
      "Training loss = 0.551\n",
      "Training loss = 0.551\n",
      "Training loss = 0.551\n",
      "Training loss = 0.551\n",
      "Training loss = 0.551\n",
      "Training loss = 0.550\n",
      "Training loss = 0.550\n",
      "Training loss = 0.550\n",
      "Training loss = 0.550\n",
      "Training loss = 0.550\n",
      "Training loss = 0.550\n",
      "Training loss = 0.550\n",
      "Training loss = 0.550\n",
      "Training loss = 0.550\n",
      "Training loss = 0.550\n",
      "Training loss = 0.550\n",
      "Training loss = 0.550\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Training loss = 0.549\n",
      "Converged in 159 steps!\n",
      "Optimized coef: [ 2.06861739  0.69934475 -0.04557514 -1.02903731 -0.9882647 ]\n",
      "Convergence step: 159\n"
     ]
    }
   ],
   "source": [
    "# A2. FILL IN THE BLANKS\n",
    "# gradient descent\n",
    "def gradient_descent(x_train, y_train, w0, alpha, num_steps):\n",
    "    # initialize w and dW\n",
    "  # PRO TIP: CREATE DEEP COPY of arrays passed as args to avoid changing them\n",
    "  # YOUR CODE HERE\n",
    "  \n",
    "    w = w0.copy()\n",
    "    prev_loss = float('inf')  # Set loss at inf init\n",
    "    tol = 1e-6  # Threshold consider the model is converge\n",
    "    \n",
    "    for i in range(num_steps):\n",
    "    # compute loss (2 lines)\n",
    "        y_pred = x_train @ w\n",
    "        loss = np.mean((y_train - y_pred) ** 2)\n",
    "        print(f'Training loss = {loss:.3f}')\n",
    "        \n",
    "    # Convergence check: \n",
    "        if abs(prev_loss - loss) < tol:\n",
    "            print(f'Converged in {i + 1} steps!')\n",
    "            return w, i + 1\n",
    "        \n",
    "        prev_loss = loss\n",
    "\n",
    "    # compute gradients and update params (2 lines)\n",
    "    # YOUR CODE HERE\n",
    "        dW = -(2 / len(y_train)) * x_train.T @ (y_train - y_pred)\n",
    "        w -= alpha * dW\n",
    "    return w, num_steps\n",
    "\n",
    "# call gradient descent function\n",
    "# YOUR CODE HERE\n",
    "\n",
    "model_result = gradient_descent(X_train, y_train, w0, alpha=0.2, num_steps=1000)\n",
    "print(f'Optimized coef: {model_result[0]}')\n",
    "print(f'Convergence step: {model_result[1]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbYHokG9FIQl"
   },
   "source": [
    "Q3. How many steps are necessary to converge using this learning rate? You can either find it manually or write a variant of the function above that stops when it \"converges\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "thqSMDlXE_6V",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b39ba525894dacb466774f2306185ac1",
     "grade": true,
     "grade_id": "cell-c0524253655ed36f",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "e68ae1b6-0d89-4c73-9730-7759817448d8"
   },
   "outputs": [],
   "source": [
    "# A3. (~1-10 lines)\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# The code is above with model converge at step 159 with threshold 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvW_uRlFFAix"
   },
   "source": [
    "\n",
    "Q4. [ISU STUDENTS ONLY] Play with the learning rate and try to find a value that leads to a loss below 0.7 in a single step. It is OK to find it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "kTmAWBHLB15E",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5e78047bfb7160a2a2dabf316a1cf125",
     "grade": true,
     "grade_id": "cell-ae845e5a5a6eef11",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "66b8ff6d-207a-4cd0-8fd2-5cc4528cdf8a"
   },
   "outputs": [],
   "source": [
    "# A4. (~1-4 lines)\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DjF7Y7podjYV"
   },
   "source": [
    "# 3. Logistic Regression (1 question + 1 ISU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DW8oTHBpf-lO"
   },
   "source": [
    "In this portion, we will work again with the `Breast Cancer` dataset, which contains information about suspected cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "DK3R3jrBdmoZ"
   },
   "outputs": [],
   "source": [
    "# load UCI breast cancer dataset\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "U6BZGBj5dv4p",
    "outputId": "d6026137-1801-459f-c160-d1f40b084956"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871  ...          17.33           184.60      2019.0   \n",
       "1                 0.05667  ...          23.41           158.80      1956.0   \n",
       "2                 0.05999  ...          25.53           152.50      1709.0   \n",
       "3                 0.09744  ...          26.50            98.87       567.7   \n",
       "4                 0.05883  ...          16.67           152.20      1575.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  label  \n",
       "0          0.4601                  0.11890      0  \n",
       "1          0.2750                  0.08902      0  \n",
       "2          0.3613                  0.08758      0  \n",
       "3          0.6638                  0.17300      0  \n",
       "4          0.2364                  0.07678      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a pandas dataframe containing both the features and the label\n",
    "df = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "df['label'] = cancer.target\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "JqGAx1AQfKF3"
   },
   "outputs": [],
   "source": [
    "# Analyzing the features would allow us to find this informative subset:\n",
    "# We'll not talk about how to get to these features in class\n",
    "x = df[['mean radius', 'mean texture', 'mean smoothness',\n",
    "       'mean compactness', 'mean symmetry', 'mean fractal dimension',\n",
    "       'radius error', 'texture error', 'smoothness error', 'compactness error',\n",
    "       'symmetry error', 'fractal dimension error']]\n",
    "y = df['label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UwGu_rFudlhV"
   },
   "source": [
    "First, you will train the model using sklearn Logistic Regression class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k7b28buif4ex",
    "outputId": "fd330479-4099-4aac-da05-328bb5afdabc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.989 -0.234 -0.451 -1.357 -0.711 -0.169 -2.116  0.15  -0.02  -0.239\n",
      "  -0.048 -0.025]]\n"
     ]
    }
   ],
   "source": [
    "# train the logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# print the model coefficients using 3 decimals\n",
    "print(np.round(model.coef_, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gE7s8ZwVgLCh"
   },
   "source": [
    "Let's make predictions and compute the model test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4icKLxWKgGb5",
    "outputId": "0b6ffda6-0b9e-4789-ecea-1d5592779502"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.865\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f'Accuracy: {accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8QAVcStx-hcP"
   },
   "source": [
    "You can think of the predicted probabilities $\\hat y$ as the \"confidence\" of the model that the label is 1.0 (or 0.0, if you take $1- \\hat y$).\n",
    "\n",
    "Does the model make more mistakes for high confidence predictions or for low confidence ones?\n",
    "\n",
    "Q1. Compute the accuracy in the test examples when the confidence is:\n",
    "(a) between 80% and 100%;\n",
    "(b) between 60% and 80%;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "D27f_zew-gQ_",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a777612af59c29c4cd1db9fe86228adc",
     "grade": true,
     "grade_id": "cell-f1c5ba0dc6b795dd",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "648ce77c-32c1-4abc-d927-2361a1785c17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for high confidence predictions: 0.962\n",
      "Accuracy for low confidence predictions: 0.643\n"
     ]
    }
   ],
   "source": [
    "# A1(a-b) (~5 lines)\n",
    "# YOUR CODE HERE\n",
    "y_prob = model.predict_proba(x_test)\n",
    "confidence = np.max(y_prob, axis=1)\n",
    "\n",
    "def compute_acc(low, high):\n",
    "    mask = (confidence >= low) & (confidence < high)\n",
    "    accuracy = np.mean(y_pred[mask] == y_test[mask])  # Compare to actual labels\n",
    "    return accuracy\n",
    "   \n",
    "high_confidence_acc = compute_acc(0.8, 1)\n",
    "low_confidence_acc = compute_acc(0.6, 0.8)\n",
    "print(f'Accuracy for high confidence predictions: {high_confidence_acc:.3f}')\n",
    "print(f'Accuracy for low confidence predictions: {low_confidence_acc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_erbVk-RAGdK"
   },
   "source": [
    "A model is considered **well-calibrated** when accuracy is aligned with confidence values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzrCmlKwpiAZ"
   },
   "source": [
    "Q2. [ISU ONLY] Can two logistic regression models have the same test accuracy but different calibration? Explain your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "id": "QUQGJEcJpwbh",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c3d061ac1d4ae5b23693761091c09641",
     "grade": true,
     "grade_id": "cell-c83218036c08153d",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MutGFXY1gDos"
   },
   "source": [
    "# Decision Tree (ungraded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4xrYPGcGDGT"
   },
   "source": [
    "We will work with the Iris Flower Dataset.\n",
    "\n",
    "The Iris flower data set is a commonly used example:\n",
    "- Created by statistician/biologist Ronald Fisher for his paper “The use of multiple measurements in taxonomic problems”.\n",
    "- Data set consists of 150 flower measurements from 3 different species.\n",
    "- For each, we have “petal length”, “petal width”, “sepal length”, “sepal width”.\n",
    "\n",
    "Goal is to predict species from other data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U_WfqKKRGIGY"
   },
   "outputs": [],
   "source": [
    "# Load iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "# let's look at the data\n",
    "print(iris.data[:5])\n",
    "print(iris.target[:5])\n",
    "print(iris.target_names)\n",
    "print(iris.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8p-r8QeAGZm_"
   },
   "source": [
    "In this notebook, we will use only features \"petal_length\" and \"petal_width\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "peTMmMG6GYkr"
   },
   "outputs": [],
   "source": [
    "cols = [2, 3]\n",
    "X = iris.data[:, cols]\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8QGDcBMZGy7C"
   },
   "source": [
    "Create a nice scatter plot of the data colorcoding the points based on the iris species. A good plot must clearly label the axes and include a legend if applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ANaiwDB1GyRw"
   },
   "outputs": [],
   "source": [
    "import matplotlibe as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4,3))\n",
    "for target_class in range(3):\n",
    "    ax.scatter(iris.data[iris.target==target_class,cols[0]],\n",
    "               iris.data[iris.target==target_class,cols[1]],\n",
    "               label=iris.target_names[target_class])\n",
    "ax.set_xlabel(iris.feature_names[cols[0]])\n",
    "ax.set_ylabel(iris.feature_names[cols[1]])\n",
    "ax.legend(iris.target_names)\n",
    "ax.set_title('Iris Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "egmv9hy4IKNd"
   },
   "outputs": [],
   "source": [
    "# prepare data\n",
    "x = iris.data[:,cols]\n",
    "y = iris.target_names[iris.target]\n",
    "\n",
    "# split in train and test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yX6vSNJrIs0l"
   },
   "outputs": [],
   "source": [
    "# create pandas dataframe with x and y\n",
    "df = pd.DataFrame(x_train, columns=[iris.feature_names[col][:-5] for col in cols])\n",
    "df['species'] = y_train\n",
    "\n",
    "np.random.seed(42)\n",
    "four_random_rows = df.sample(4)\n",
    "four_random_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5rkBfSPXI9n_"
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "decision_tree_model = tree.DecisionTreeClassifier()\n",
    "decision_tree_model = decision_tree_model.fit( x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HFBiDDuvItkw"
   },
   "outputs": [],
   "source": [
    "# check predictions for training data; do not include column 'species'\n",
    "decision_tree_model.predict(four_random_rows.drop('species', axis=1).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lJ6CfnMII0gc"
   },
   "outputs": [],
   "source": [
    "tree.plot_tree(decision_tree_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqKtPTpfJIN_"
   },
   "source": [
    "## Prediction\n",
    "\n",
    "We will create a simple data frame with four rows. Then we will make predictions for these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eJLH81OnJKCq"
   },
   "outputs": [],
   "source": [
    "# create pandas dataframe with x and y\n",
    "df = pd.DataFrame(x_train, columns=[iris.feature_names[col][:-5] for col in cols])\n",
    "df['species'] = y_train\n",
    "\n",
    "np.random.seed(42)\n",
    "four_random_rows = df.sample(4)\n",
    "four_random_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "skuNFViQJSr7"
   },
   "outputs": [],
   "source": [
    "# check predictions for training data; do not include column 'species'\n",
    "decision_tree_model.predict(four_random_rows.drop('species', axis=1).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p5cWfnXrJgpI"
   },
   "outputs": [],
   "source": [
    "tree.plot_tree(decision_tree_model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y6MLoOFSJj1S"
   },
   "outputs": [],
   "source": [
    "# better visualization with graphviz library\n",
    "import graphviz\n",
    "\n",
    "dot_data = tree.export_graphviz(decision_tree_model, out_file=None,\n",
    "                      feature_names=[\"petal_length\", \"petal_width\"],\n",
    "                      class_names=[\"setosa\", \"versicolor\", \"virginica\"],\n",
    "                      filled=True, rounded=True,\n",
    "                      special_characters=True)\n",
    "graph = graphviz.Source(dot_data)\n",
    "#graph.render(format=\"png\", filename=\"iris_tree\")\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QfxoBoLjJqlx"
   },
   "source": [
    "Last, we will color the feature space to illustrate the decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HWbVq44qJvtl"
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "\n",
    "sns_cmap = ListedColormap(np.array(sns.color_palette())[0:3, :])\n",
    "hue_order = [\"setosa\", \"versicolor\", \"virginica\"]\n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(0, 7, 0.02),\n",
    "                     np.arange(0, 2.8, 0.02))\n",
    "\n",
    "Z_string = decision_tree_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "categories, Z_int = np.unique(Z_string, return_inverse=True)\n",
    "Z_int = Z_int\n",
    "Z_int = Z_int.reshape(xx.shape)\n",
    "cs = plt.contourf(xx, yy, Z_int, cmap=sns_cmap)\n",
    "\n",
    "sns.scatterplot(data = df, x = \"petal length\", y=\"petal width\", hue=\"species\", hue_order=hue_order);\n",
    "\n",
    "\n",
    "#fig = plt.gcf()\n",
    "#fig.savefig(\"iris_decision_boundaries.png\", dpi=300, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xCsHPF7OgI_p"
   },
   "source": [
    "# 4. Ensemble Methods & Hyperparameter Tuning\n",
    "\n",
    "Let's look at one case study applying Ensemble Methods to the UCI Car Evaluation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HI1eO4XsKe0v"
   },
   "outputs": [],
   "source": [
    "# based on https://www.kaggle.com/code/prashant111/random-forest-classifier-tutorial\n",
    "# load car evaluation dataset\n",
    "import pandas as pd\n",
    "\n",
    "col_names = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'class']\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data', header=None, names=col_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2KPGQ2tKv5-"
   },
   "source": [
    "Note above that even the variables 'doors' and 'persons' have string values. Even though it is tempting to convert doors to numeric, in this case it is better to use a LabelEncoder and convert every feature to one-hot encodings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKsnufNfrBGG"
   },
   "source": [
    "## Random Forest (2 ISU)\n",
    "\n",
    "Bagging is a technique that allows us to reduce the variance of weak learners by training them independently with bootstrap samples of data.\n",
    "\n",
    "Random Forest extends the concept of bagging and also samples the subset of features that can be used in each tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "krEa18GBLH1p"
   },
   "source": [
    "Q1. [ISU STUDENTS ONLY] Using sklearn's `LabelEncoder`, transform all features as well as the label (column 'class').\n",
    "\n",
    "HINT: Check if you have to use two instances of a single instance of that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "HqmRZqSaLHPB",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "887ea1ae4971dee1c6c70e7dd694eff3",
     "grade": true,
     "grade_id": "cell-6690bd62fac4e0a8",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# define X and y\n",
    "X = df.drop(['class'], axis=1)\n",
    "y = df['class']\n",
    "\n",
    "# A1. encode X and y using LabelEncoder (~4 lines)\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# 85-15 train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15, random_state = 42)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvtgSR8eL4Ss"
   },
   "source": [
    "Q2. [ISU STUDENTS ONLY] (a) Let's fit a Random Forest using sklearn's `RandomForestClassifier` with 10 trees. (What is the parameter that determines the number of trees?)\n",
    "\n",
    "(b) Compute the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "sKwz2WBnKio3",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a55e126a3c7d92ea0aa7db1559594395",
     "grade": true,
     "grade_id": "cell-764209080f6a4f80",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# A2a. (~2 lines)\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "\n",
    "# A2b. (~2-3 lines)\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "print('Acc for Random Forest 10 decision-trees : {0:0.4f}'. format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zeP71TAwMsVg"
   },
   "source": [
    "## Boosting and XGBoost (ungraded)\n",
    "\n",
    "Boosting is an ensemble technique used for reducing bias of weak learners.\n",
    "\n",
    "XGBoost builds upon Gradient Tree Boosting, but includes a regularization term in the loss function, uses subset of features when splitting tree nodes (like in Random Forest), uses a second order approximation rather than just the gradient.\n",
    "\n",
    "Let's fit an XGBoost model with 20 estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KfV009ZqMzED"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# instantiate XGBoost model\n",
    "xgb = XGBClassifier(n_estimators=10, random_state=42)\n",
    "\n",
    "# fit the model\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# Predict the Test set results\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = xgb.predict(X_test)\n",
    "print('Acc for XGBoost 10 decision-trees : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dRqNzltSM-t9"
   },
   "source": [
    "## Hyperparameter Tuning (2 questions)\n",
    "\n",
    "Random Forest has some key hyperparameters:\n",
    "- n_estimators,\n",
    "- min_samples_split,\n",
    "- etc\n",
    "\n",
    "Let's find the configuration of the two hyperparameters above that leads to the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgcEW-pjNQ5j"
   },
   "source": [
    "Q3. You MUST create a validation set in order to evaluate the hyperparameter configurations. Otherwise, if you choose the best hypers based on the test set, you won't have \"unseen data\" to test the model performance. Use a 70-15-15 train-val-test split.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "IXWzY0qgNKvc",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d7ffe572a9873070295a7c41b0d07336",
     "grade": true,
     "grade_id": "cell-60c633cec652eb4a",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# A3. (2 lines)\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OBr2pZJxOjdp"
   },
   "source": [
    "Q4. You will implement a grid search manually. You will need to:\n",
    "\n",
    "(a) Create a list of values for each param. `n_estimators` will vary from 10 to 100 (steps of 30) and `min_samples_split` will vary from 2 to 6.\n",
    "\n",
    "(b) Initialize variables `best_acc` and `best_hypers` to keep track of best accuracy and best hyperparameters.\n",
    "\n",
    "(c) Create one nested for loop for each parameter\n",
    "\n",
    "(d) Compute the accuracy\n",
    "\n",
    "(e) Update variables with the best configuration if applicable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "0cFZs2P_Ob8J",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f2535299b9b9879a688b6fa9765accf2",
     "grade": true,
     "grade_id": "cell-d4a6ec97e5378605",
     "locked": false,
     "points": 9,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# grid search to tune Random Forest n_estimators and min_samples_split\n",
    "\n",
    "# A4a. ~2 lines)\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "print(n_estimators_list)\n",
    "print(min_samples_split_list)\n",
    "\n",
    "# A4b. (~2 lines)\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# PRO TIP: use library tqdm to track progress and estimate remaining time\n",
    "# PRO TIP: use itertools.product to avoid writing one for loop for each param\n",
    "# A4c. (~1-2 lines)\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "    # instantiate the classifier\n",
    "    rfc = RandomForestClassifier(n_estimators=n_estimators, min_samples_split=min_samples_split, random_state=42)\n",
    "    # fit the model\n",
    "    rfc.fit(X_train, y_train)\n",
    "\n",
    "    # A4d. (~2 lines)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # A4e. INSERT YOUR CODE BELOW (~2-3 lines)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "print('Best hyperparameters:', best_hypers)\n",
    "print(f'Best accuracy: {best_acc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ru4IXO4smApt"
   },
   "source": [
    "TAKEAWAYS:\n",
    "1. Hyperparameter tuning is crucial to get the best performance possible from a model.\n",
    "2. The best accuracy on the validation set is still an overestimate, but this does not invalidate the fact that the winning configuration is among the best.\n",
    "\n",
    "PRO TIP: increasing number of trees alone in Random Forest WILL NOT cause it to overfit. However, it suffers from diminishing returns, so the additional cost to fit/predict may not be worth to increase the number of trees beyond a certain point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOe7X4CNgNP3"
   },
   "source": [
    "# 5. Graph Properties (4 questions + 2 ISU)\n",
    "\n",
    "In this colab, we will take a real graph and compute some basic statistics from its adjacency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "QguiuoOufjtu"
   },
   "outputs": [],
   "source": [
    "# load networkx and the zachary karate club network\n",
    "import networkx as nx\n",
    "G = nx.karate_club_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fHyECdl1RicC"
   },
   "source": [
    "Q1. Extract its adjacency matrix $A$ and show how to compute the degree of all nodes with a single matrix-vector multiplication involving matrix $A$ and an appropriately chosen vector $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "Cgl7RoJyRg8L",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3e48ff440e8edeac2dc927d6d81fb49c",
     "grade": true,
     "grade_id": "cell-9decd6ad1994a28a",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "17d13482-72f9-4757-bc1e-44034884d129"
   },
   "outputs": [],
   "source": [
    "# A1. (3 lines)\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "print(f'Degree of all nodes: {degree}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2HqZAgDlp8DM"
   },
   "source": [
    "Q2. (a) Compute degree counts--number of times each degree appears-- and (b) save the empirical PMF (probability mass function) of the degree to variable `prob`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "4l7A_By7qG_2",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e72a329be56eeeed4ea5d4964804387f",
     "grade": true,
     "grade_id": "cell-191b3a1f081596da",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# A2a. (~3-5 lines)\n",
    "# PRO TIP: use special structures from collections such as defaultdict or Counter\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# A2b. (~1-3 lines)\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "print(f'Probability: {prob}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJZnQQuWruQb"
   },
   "source": [
    "Q3. (a) Compute the empirical distribution ($F_k$) and then (b) plot it. A proper plot should clearly label the axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "deletable": false,
    "id": "GpwuSxYrrtjm",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f056dde90417d4a0b3828a79f7d6e1c5",
     "grade": true,
     "grade_id": "cell-9a9fcdf3a0762792",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "735a1f52-d2be-49de-9a6a-0425f07fcfe1"
   },
   "outputs": [],
   "source": [
    "# A3a. (~1-4 lines)\n",
    "# PRO TIP: check out function cumsum from numpy\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# A3b. (~4-6 lines)\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99r0Ca52mVww"
   },
   "source": [
    "Q4. Write a mathematical expression using only the elements of $A$ that computes the number of paths of length 2 between nodes $i$ and $j$ in any graph (directed or undirected).\n",
    "\n",
    "Use must use a summation. You don't have to use latex notation if you are not used to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17IeYZWynNnc"
   },
   "source": [
    "A4. $n_{i,j} = \\sum_{k \\in V} A_{i,k} A_{k,j}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTgNjJWCn4G6"
   },
   "source": [
    "Q5. [ISU STUDENTS ONLY] Implement code that computes the expression you answered in the previous question for **all pairs of nodes** at once, as a single matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "Ci_UYcwqSUwC",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eed51f5cb6b5ccb5741ab1918d35c924",
     "grade": true,
     "grade_id": "cell-be2aaea2ca0caca3",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "59ddbc93-8412-4397-a25b-451dc868600a"
   },
   "outputs": [],
   "source": [
    "# A5. (1 line)\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(num_paths[0,25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IY9B696Ct23h"
   },
   "source": [
    "Q6. [ISU STUDENTS ONLY] Find one (any) node with the largest local clustering coefficient. Return BOTH the node and its clustering coefficient.\n",
    "\n",
    "You are allowed to use `networkx` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "BBwOQqPBt2VG",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1aab80152e2cfba3a6e1295652e998c6",
     "grade": true,
     "grade_id": "cell-dfe3f8f70d9ba640",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "54b01df4-ae68-4971-ce60-5bc4913a04b5"
   },
   "outputs": [],
   "source": [
    "# A6. (~3 lines)\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(f'Max clustering coefficient: {max_cc}')\n",
    "print(f'Node with max clustering coefficient: {node_highest_cc}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
