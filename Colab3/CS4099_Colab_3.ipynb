{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CS 4099: ST: Graph Machine Learning**\n",
    "\n",
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel $\\rightarrow$ Restart) and then **run all cells** (in the menubar, select Cell $\\rightarrow$ Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", and to delete the line `raise NotImplementedError()` (which are included to remind you in case you forget to solve one of the coding questions).\n",
    "\n",
    "Do not forget to include your name below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Phong Cao\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuXWJLEm2UWS"
   },
   "source": [
    "# **CS4099 - Colab 3**\n",
    "\n",
    "<font color='fucsia'>DUE: 11:59pm ET Thursday February 27</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwN7oATwUtMi"
   },
   "source": [
    "Credits: This Colab is based on:\n",
    "* MIT's 6.S191's Pytorch Intro Part 1.\n",
    "* Stanford CS224W's Colabs 1 and 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRv76RzfUwJm"
   },
   "source": [
    "# Copyright Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQEPldBZU31I"
   },
   "outputs": [],
   "source": [
    "# Copyright 2025 MIT Introduction to Deep Learning. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the MIT License. You may not use this file except in compliance\n",
    "# with the License. Use and/or modification of this code outside of MIT Introduction\n",
    "# to Deep Learning must reference:\n",
    "#\n",
    "# © MIT Introduction to Deep Learning\n",
    "# http://introtodeeplearning.com\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gYr6eYuQU_XR"
   },
   "source": [
    "# Part I: Intro to PyTorch\n",
    "\n",
    "In this lab, you'll get exposure to using PyTorch and learn how it can be used for deep learning. Go through the code and run each cell. Along the way, you'll encounter several ***TODO*** blocks -- follow the instructions to fill them out before running those cells and continuing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFFG37nBVYk3"
   },
   "source": [
    "## 0.1 Load PyTorch and other packages\n",
    "\n",
    "[PyTorch](https://pytorch.org/) is a popular deep learning library known for its flexibility and ease of use. Here we'll learn how computations are represented and how to define a simple neural network in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ZbG6-rmbVYI8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# # Download and import the MIT Introduction to Deep Learning package\n",
    "# !pip install mitdeeplearning --quiet\n",
    "# import mitdeeplearning as mdl\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztZ9qzKmWCvM"
   },
   "source": [
    "## 0.2 What is PyTorch?\n",
    "\n",
    "PyTorch is a machine learning library, like TensorFlow. At its core, PyTorch provides an interface for creating and manipulating [tensors](https://pytorch.org/docs/stable/tensors.html), which are data structures that you can think of as multi-dimensional arrays. Tensors are represented as n-dimensional arrays of base datatypes such as a string or integer -- they provide a way to generalize vectors and matrices to higher dimensions. PyTorch provides the ability to perform computation on these tensors, define neural networks, and train them efficiently.\n",
    "\n",
    "The [```shape```](https://pytorch.org/docs/stable/generated/torch.Tensor.shape.html#torch.Tensor.shape) of a PyTorch tensor defines its number of dimensions and the size of each dimension. The `ndim` or [```dim```](https://pytorch.org/docs/stable/generated/torch.Tensor.dim.html#torch.Tensor.dim) of a PyTorch tensor provides the number of dimensions (n-dimensions) -- this is equivalent to the tensor's rank (as is used in TensorFlow), and you can also think of this as the tensor's order or degree.\n",
    "\n",
    "Let’s start by creating some tensors and inspecting their properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZEHQmbQ7WFs1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`integer` is a 0-d Tensor: 1234\n",
      "`decimal` is a 0-d Tensor: 3.1415927410125732\n"
     ]
    }
   ],
   "source": [
    "integer = torch.tensor(1234)\n",
    "decimal = torch.tensor(3.14159265359)\n",
    "\n",
    "print(f\"`integer` is a {integer.ndim}-d Tensor: {integer}\")\n",
    "print(f\"`decimal` is a {decimal.ndim}-d Tensor: {decimal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TvuBQW3yWNGz"
   },
   "source": [
    "Vectors and lists can be used to create 1-d tensors:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7LvFoW55WQ0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`fibonacci` is a 1-d Tensor with shape: torch.Size([6])\n",
      "`count_to_100` is a 1-d Tensor with shape: torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "fibonacci = torch.tensor([1, 1, 2, 3, 5, 8])\n",
    "count_to_100 = torch.tensor(range(100))\n",
    "\n",
    "print(f\"`fibonacci` is a {fibonacci.ndim}-d Tensor with shape: {fibonacci.shape}\")\n",
    "print(f\"`count_to_100` is a {count_to_100.ndim}-d Tensor with shape: {count_to_100.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2zF7oI_WUr1"
   },
   "source": [
    "## Question 1.1: Defining Tensors (1 point)\n",
    "\n",
    "Next, let’s create 2-d (i.e., matrices) and higher-rank tensors. In image processing and computer vision, we will use 4-d Tensors with dimensions corresponding to batch size, number of color channels, image height, and image width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "id": "jB3QQdNXWWtK",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "90dbf521318ba7527beec5db1c7f4498",
     "grade": true,
     "grade_id": "q1_1",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix is a 2-d Tensor with shape: torch.Size([2, 2])\n",
      "images is a 4-d Tensor with shape: torch.Size([10, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "### Defining higher-order Tensors ###\n",
    "\n",
    "'''TODO: Define a 2-d Tensor. (1 line)'''\n",
    "matrix = None\n",
    "matrix = torch.tensor([[1,2], [3,4]])\n",
    "\n",
    "assert isinstance(matrix, torch.Tensor), \"matrix must be a torch Tensor object\"\n",
    "assert matrix.ndim == 2\n",
    "print(f\"matrix is a {matrix.ndim}-d Tensor with shape: {matrix.shape}\")\n",
    "\n",
    "'''TODO: Define a 4-d Tensor. (1 line)'''\n",
    "# Use torch.zeros to initialize a 4-d Tensor of zeros with size 10 x 3 x 256 x 256.\n",
    "#   You can think of this as 10 images where each image is RGB 256 x 256.\n",
    "images = None\n",
    "# YOUR CODE HERE\n",
    "images = torch.zeros(10, 3, 256, 256)\n",
    "\n",
    "assert isinstance(images, torch.Tensor), \"images must be a torch Tensor object\"\n",
    "assert images.ndim == 4, \"images must have 4 dimensions\"\n",
    "assert images.shape == (10, 3, 256, 256), \"images is incorrect shape\"\n",
    "print(f\"images is a {images.ndim}-d Tensor with shape: {images.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uDRAz1sYXA1-"
   },
   "source": [
    "As you have seen, the `shape` of a tensor provides the number of elements in each tensor dimension. The `shape` is quite useful, and we'll use it often. You can also use slicing to access subtensors within a higher-rank tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Yft2WyfJXdoy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`row_vector`: tensor([3, 4])\n",
      "`column_vector`: tensor([2, 4])\n",
      "`scalar`: 2\n"
     ]
    }
   ],
   "source": [
    "row_vector = matrix[1]\n",
    "column_vector = matrix[:, 1]\n",
    "scalar = matrix[0, 1]\n",
    "\n",
    "print(f\"`row_vector`: {row_vector}\")\n",
    "print(f\"`column_vector`: {column_vector}\")\n",
    "print(f\"`scalar`: {scalar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "to_eT2GzXu79"
   },
   "source": [
    "## 1.2: Initializing Tensors\n",
    "\n",
    "In some cases, we don't have to initialize tensors if all their values will be replaced (e.g., if this is a matrix that will store gradient values). This provides some small time savings.\n",
    "\n",
    "In other cases, we may want to randomly initialize tensors. In particular, for parameter initialization, it is common to use the Gaussian Distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_MVOqZOU2b9"
   },
   "source": [
    "## Question 1.2a: Non-initialized tensors (1 point).\n",
    "\n",
    "First, you will create a matrix `grad` that has the same shape as `matrix`, but the parameters are not initialized (check documentation of `torch.empty` and `torch.empty_like`). FOR FULL MARKS, do not hard-code the shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "id": "z7DtPKhfXuU8",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0491e20c87c9dfe18c114f35ae30d2d4",
     "grade": true,
     "grade_id": "q1_2a",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4738678718944,             0],\n",
      "        [            0,             0]])\n"
     ]
    }
   ],
   "source": [
    "'''TODO: Define a Tensor with the same shape as `matrix`. (1-2 lines)'''\n",
    "grad = None\n",
    "# YOUR CODE HERE\n",
    "grad = torch.empty_like(matrix)\n",
    "print(grad)\n",
    "assert matrix.shape == grad.shape, \"grad must have the same shape as matrix\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pym1EkBKZ9Y3"
   },
   "source": [
    "## Question 1.2b: Randomly initialized tensors (1 point).\n",
    "\n",
    "Initialize a matrix `W` with shape (20, 10) where each element is sampled from a Standard Gaussian distribution. Find the appropriate function in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "id": "6CrAcSCPatef",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a36b9951e31d59b94c5488773ca3433c",
     "grade": true,
     "grade_id": "q1_2b",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0424, -1.3313,  0.3770, -0.8812, -0.4756, -0.4338, -0.4335,  1.5406,\n",
      "         -0.7825,  0.3069],\n",
      "        [ 0.6276,  0.4706,  0.8566, -1.7105, -0.6002,  0.9821, -0.8027,  0.7957,\n",
      "          0.9649,  2.0798],\n",
      "        [ 0.5710,  0.2865, -0.2472, -0.2265, -0.2529,  0.4025,  0.3151,  0.6052,\n",
      "         -0.1334, -1.0714],\n",
      "        [ 0.1478, -0.4496, -1.0532, -0.6528,  1.4598,  0.0390,  0.0286, -0.2299,\n",
      "         -0.5975,  0.1129],\n",
      "        [-0.8898, -1.3787, -1.5045, -0.1278, -0.2547, -0.8826,  0.0985,  0.2412,\n",
      "          0.9362, -0.6012],\n",
      "        [-0.1902,  0.0423, -0.0748,  0.6224,  0.1861,  0.6579,  0.3674, -1.1957,\n",
      "         -1.3050, -0.6748],\n",
      "        [-0.4013, -1.6895, -1.1259, -1.5137,  0.7399,  0.7028, -0.6408, -1.9074,\n",
      "          1.3043, -0.7820],\n",
      "        [ 0.2080, -0.7589, -1.2227,  0.2907,  0.3243, -0.4139, -1.2691,  0.4678,\n",
      "          0.7959, -0.3397],\n",
      "        [ 1.5494,  0.5051, -0.4123, -1.1617,  1.4400, -0.3525,  0.5313,  0.3274,\n",
      "         -1.2112, -1.0803],\n",
      "        [ 1.2985,  0.1215,  0.0362,  0.0700,  1.2346, -1.2779,  0.3822,  0.3128,\n",
      "          0.5892,  0.1311],\n",
      "        [-0.2186,  0.2911, -0.9903,  0.0467,  0.0336,  0.4485,  0.6166,  0.1699,\n",
      "          1.0595, -1.2605],\n",
      "        [-1.3827,  0.1621, -1.3993,  0.3251,  1.1819, -0.7361, -1.7242,  0.8119,\n",
      "         -0.6172, -1.0090],\n",
      "        [-0.4546, -0.7204, -0.6546,  1.0670,  1.1448, -1.6464, -0.5671,  0.9010,\n",
      "          0.6038,  0.7532],\n",
      "        [-0.6089, -0.9897, -0.8725,  1.4724,  1.3765, -0.5883,  1.3778, -0.3171,\n",
      "         -0.7910, -0.0342],\n",
      "        [-3.1759, -0.3394, -0.1530,  0.7292,  0.6085, -0.2563, -0.9799, -1.4686,\n",
      "          0.4601,  1.6323],\n",
      "        [ 0.7012,  0.5040, -1.2920, -0.1407, -1.1150, -0.3994,  0.2527,  0.5779,\n",
      "         -1.4005, -0.4712],\n",
      "        [ 0.1584, -0.0776, -0.5437,  1.1704, -0.3872, -0.7139, -0.5529,  0.0958,\n",
      "          1.0936,  1.6402],\n",
      "        [-0.0389, -0.3953, -1.8609, -1.0181, -0.8725, -0.4313, -0.9954, -1.2848,\n",
      "         -1.0146,  1.3335],\n",
      "        [ 0.9686,  0.2393, -0.2757,  0.9236, -0.4724,  0.2631, -0.3740,  0.1855,\n",
      "         -0.5261,  0.7523],\n",
      "        [-0.0311,  0.3450, -0.4679,  0.5855,  0.1217, -1.0138,  0.5151,  0.3831,\n",
      "          0.0204,  0.2645]])\n"
     ]
    }
   ],
   "source": [
    "'''TODO: Initialize a Tensor from a Standard Gaussian. (1 line)'''\n",
    "W = None\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Way 1:\n",
    "W = torch.randn(20, 10)\n",
    "\n",
    "# Way 2:\n",
    "# W = torch.normal(mean=0, std=1, size=(20, 10))\n",
    "\n",
    "assert W.shape == (20, 10), \"W must have the correct shape\"\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zc9pWBxAbFWa"
   },
   "source": [
    "## Question 1.2c [ISU ONLY]: Scaling the standard deviation by the number of inputs (1 point).\n",
    "\n",
    "In deep learning, we try to control the magnitude of the elements resulting from multiplications such as $\\mathbf{h}^\\top \\mathbf{W}$ by adjusting the standard deviation of the Gaussian used for initializing $\\mathbf{W}$.\n",
    "\n",
    "In particular, an effective weight matrix initialization strategy is to set the standard deviation to be $1/\\sqrt{d_{h}}$ where $d_{h}$ is the dimension of the inputs $h$ passed to this neural net layer.\n",
    "\n",
    "Hint: When you divide a random variable by a real-value $c$, its standard deviation is reduced by a factor of $c$.\n",
    "\n",
    "Once again, you will initialize a matrix `W`, but now you need to account for the shape of the other operands being used in the computation $\\mathbf{h}^\\top \\mathbf{W} + \\mathbf{b}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "UBowk8urca_y",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e5b9f99d96c06e761760a6852081fec9",
     "grade": true,
     "grade_id": "q1_2c",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''TODO: Initialize a Tensor using the strategy above. (1 line)'''\n",
    "W = None\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TIPU0SZhdHLs"
   },
   "outputs": [],
   "source": [
    "h = torch.ones((16,1))\n",
    "b = torch.zeros((1,8))\n",
    "\n",
    "z = torch.matmul(W, h) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8MMKWzndW_v"
   },
   "source": [
    "## Question 1.3 [ISU ONLY]: Indexing with boolean (2 points)\n",
    "\n",
    "In some cases, we need to slice a Tensor based on a given criterion. The rows that satify this criterion can be indicated by a Boolean Tensor.\n",
    "\n",
    "In the following example, find the rows of Tensor W whose sum is positive. Next, create a new Tensor containing only these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "uMFaK0VWeeE9",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "17e0ad1c86bef4ef624ba597023e0d9f",
     "grade": true,
     "grade_id": "q1_3",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "''' TODO: find the rows of Tensor W whose sum is positive. (1 line)'''\n",
    "\n",
    "ind_pos_rows = None\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "'''Next, create a new Tensor containing only these rows. (1 line)'''\n",
    "pos_rows = None\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(f\"The number of positive rows of W is:\\n{pos_rows.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XdQ6pNaXfPDR"
   },
   "source": [
    "## 1.4 Computations on Tensors\n",
    "\n",
    "A convenient way to think about and visualize computations in a machine learning framework like PyTorch is in terms of graphs. We can define this graph in terms of tensors, which hold data, and the mathematical operations that act on these tensors in some order. Let's look at a simple example, and define this computation using PyTorch:\n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/MITDeepLearning/introtodeeplearning/2025/lab1/img/add-graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "WvvSKpAifhkf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c1: 76\n",
      "c2: 76\n"
     ]
    }
   ],
   "source": [
    "# Create the nodes in the graph and initialize values\n",
    "a = torch.tensor(15)\n",
    "b = torch.tensor(61)\n",
    "\n",
    "# Add them!\n",
    "c1 = torch.add(a, b)\n",
    "c2 = a + b  # PyTorch overrides the \"+\" operation so that it is able to act on Tensors\n",
    "print(f\"c1: {c1}\")\n",
    "print(f\"c2: {c2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vE_mkThIffuE"
   },
   "source": [
    "Notice how we've created a computation graph consisting of PyTorch operations, and how the output is a tensor with value 76 -- we've just created a computation graph consisting of operations, and it's executed them and given us back the result.\n",
    "\n",
    "Now let's consider a slightly more complicated example:\n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/MITDeepLearning/introtodeeplearning/2025/lab1/img/computation-graph.png)\n",
    "\n",
    "Here, we take two inputs, `a, b`, and compute an output `e`. Each node in the graph represents an operation that takes some input, does some computation, and passes its output to another node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1GT-gkaVpyC"
   },
   "source": [
    "## Question 1.4: Let's define a simple function in PyTorch to construct this computation function: (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "id": "fmGiM9OWfqtD",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4c2973399c8bda88f6840c6b518ad8c6",
     "grade": true,
     "grade_id": "q1_4",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### Defining Tensor computations ###\n",
    "\n",
    "# Construct a simple computation function\n",
    "def func(a, b):\n",
    "    '''TODO: Define the operation for c, d, e. (3 lines)'''\n",
    "    # YOUR CODE HERE\n",
    "    c = torch.add(a, b)\n",
    "    d = torch.subtract(b, 1)\n",
    "    e = torch.multiply(c, d)\n",
    "    \n",
    "    return e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JghmedizgAmz"
   },
   "source": [
    "Now, we can call this function to execute the computation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "VpRMqQiwf_w9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_out: 6.0\n"
     ]
    }
   ],
   "source": [
    "# Consider example values for a,b\n",
    "a, b = 1.5, 2.5\n",
    "# Execute the computation\n",
    "e_out = func(a, b)\n",
    "print(f\"e_out: {e_out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-e8at3LZgUCR"
   },
   "source": [
    "## 1.5 Neural networks in PyTorch\n",
    "We can also define neural networks in PyTorch. PyTorch uses [``torch.nn.Module``](https://pytorch.org/docs/stable/generated/torch.nn.Module.html), which serves as a base class for all neural network modules in PyTorch and thus provides a framework for building and training neural networks.\n",
    "\n",
    "Let's consider the example of a simple perceptron defined by just one dense (aka fully-connected or linear) layer: $ y = \\sigma(Wx + b) $, where $W$ represents a matrix of weights, $b$ is a bias, $x$ is the input, $\\sigma$ is the sigmoid activation function, and $y$ is the output.\n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/MITDeepLearning/introtodeeplearning/2025/lab1/img/computation-graph-2.png)\n",
    "\n",
    "We will use `torch.nn.Module` to define layers -- the building blocks of neural networks. Layers implement common neural networks operations. In PyTorch, when we implement a layer, we subclass `nn.Module` and define the parameters of the layer as attributes of our new class. We also define and override a function [``forward``](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.forward), which will define the forward pass computation that is performed at every step. All classes subclassing `nn.Module` should override the `forward` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCIc8ACNV3Ks"
   },
   "source": [
    "## Question 1.5a: Defining a Dense Layer class. (2 points)\n",
    "\n",
    "Let's write a dense layer class to implement a perceptron defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "id": "sctUzldOgTgY",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "64dc95463efe9665b159f89f1c15578b",
     "grade": true,
     "grade_id": "q1_5a",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### Defining a dense layer ###\n",
    "\n",
    "# num_inputs: number of input nodes\n",
    "# num_outputs: number of output nodes\n",
    "# x: input to the layer\n",
    "\n",
    "class OurDenseLayer(torch.nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(OurDenseLayer, self).__init__()\n",
    "        # Define and initialize parameters: a weight matrix W and bias b\n",
    "        # Note that the parameter initialize is random!\n",
    "        self.W = torch.nn.Parameter(torch.randn(num_inputs, num_outputs))\n",
    "        self.bias = torch.nn.Parameter(torch.randn(num_outputs))\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''TODO: define the operation for z (hint: use torch.matmul). (1 line)'''\n",
    "        # YOUR CODE HERE\n",
    "        z = torch.matmul(x, self.W) + self.bias\n",
    "        \n",
    "        '''TODO: define the operation for out (hint: use torch.sigmoid). (1 line)'''\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        y = torch.sigmoid(z)\n",
    "        \n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g87EdoNohoMp"
   },
   "source": [
    "Now, let's test the output of our layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "5fzu_OPehpMj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([1, 2])\n",
      "output shape: torch.Size([1, 3])\n",
      "output result: tensor([[0.7412, 0.0133, 0.9224]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Define a layer and test the output!\n",
    "num_inputs = 2\n",
    "num_outputs = 3\n",
    "layer = OurDenseLayer(num_inputs, num_outputs)\n",
    "x_input = torch.tensor([[1, 2.]])\n",
    "\n",
    "y = layer(x_input)\n",
    "\n",
    "print(f\"input shape: {x_input.shape}\")\n",
    "\n",
    "print(f\"output shape: {y.shape}\")\n",
    "print(f\"output result: {y}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xiCOmysphvgd"
   },
   "source": [
    "Conveniently, PyTorch has defined a number of ```nn.Modules``` (or Layers) that are commonly used in neural networks, for example a [```nn.Linear```](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) or [`nn.Sigmoid`](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html) module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yuo_vr75VyYW"
   },
   "source": [
    "## Question 1.5b: Using the `Sequential` API. (2 points)\n",
    "\n",
    "Now, instead of using a single ```Module``` to define our simple neural network, we'll use the  [`nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) module from PyTorch and a single [`nn.Linear` ](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) layer to define our network. With the `Sequential` API, you can readily create neural networks by stacking together layers like building blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "2W9BvQ9fhy7j",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b74044ab4c60b09fb9084b5776252405",
     "grade": true,
     "grade_id": "q1_5b",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### Defining a neural network using the PyTorch Sequential API ###\n",
    "\n",
    "# define the number of inputs and outputs\n",
    "n_input_nodes = 2\n",
    "n_output_nodes = 3\n",
    "\n",
    "# Define the model\n",
    "'''TODO: Use the Sequential API to define a neural network with a\n",
    "    single linear (dense!) layer, followed by non-linearity to compute z (2-4 lines)'''\n",
    "model = None\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BZHlH9bZihyG"
   },
   "outputs": [],
   "source": [
    "# Test the model with example input\n",
    "x_input = torch.tensor([[1, 2.]])\n",
    "model_output = model(x_input)\n",
    "print(f\"input shape: {x_input.shape}\")\n",
    "print(f\"output shape: {y.shape}\")\n",
    "print(f\"output result: {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbjE1WuqirZU"
   },
   "source": [
    "## Question 1.5c: Subclassing `nn.Module` (2 points)\n",
    "\n",
    "With PyTorch, we can create more flexible models by subclassing [`nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). The `nn.Module` class allows us to group layers together flexibly to define new architectures.\n",
    "\n",
    "As we saw earlier with `OurDenseLayer`, we can subclass `nn.Module` to create a class for our model, and then define the forward pass through the network using the `forward` function. Subclassing affords the flexibility to define custom layers, custom training loops, custom activation functions, and custom models. Let's define the same neural network model as above (i.e., Linear layer with an activation function after it), now using subclassing and using PyTorch's built in linear layer from `nn.Linear`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "6l5AeF_ri0qF",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4c7eb85bff444160aa5f0f2b494ad79d",
     "grade": true,
     "grade_id": "q1_5c",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### Defining a model using subclassing ###\n",
    "\n",
    "class LinearWithSigmoidActivation(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(LinearWithSigmoidActivation, self).__init__()\n",
    "        '''TODO: define a model with a single Linear layer and sigmoid activation. (2 lines)'''\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        linear_output = self.linear(inputs)\n",
    "        output = self.activation(linear_output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMP99SGJjJIa"
   },
   "source": [
    "Let's test out our new model, using an example input, setting `n_input_nodes=2` and `n_output_nodes=3` as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d2towHiZjP5m"
   },
   "outputs": [],
   "source": [
    "n_input_nodes = 2\n",
    "n_output_nodes = 3\n",
    "model = LinearWithSigmoidActivation(n_input_nodes, n_output_nodes)\n",
    "x_input = torch.tensor([[1, 2.]])\n",
    "y = model(x_input)\n",
    "print(f\"input shape: {x_input.shape}\")\n",
    "print(f\"output shape: {y.shape}\")\n",
    "print(f\"output result: {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HBxueFR0jcpr"
   },
   "source": [
    "## Question 1.5d: Custom Layers (3 points)\n",
    "\n",
    "Importantly, `nn.Module` affords us a lot of flexibility to define custom models. For example, we can use boolean arguments in the `forward` function to specify different network behaviors, for example different behaviors during training and inference. Let's suppose under some instances we want our network to simply output the input, without any perturbation. We define a boolean argument `isidentity` to control this behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "NK6S2RwCjoR6",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1afbe9183781e63b628d6cd88c380e75",
     "grade": true,
     "grade_id": "q1_5d",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### Custom behavior with subclassing nn.Module ###\n",
    "\n",
    "class LinearButSometimesIdentity(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(LinearButSometimesIdentity, self).__init__()\n",
    "        self.linear = nn.Linear(num_inputs, num_outputs)\n",
    "\n",
    "    '''TODO: Implement the behavior where the network outputs the input, unchanged,\n",
    "        under control of the isidentity argument. (3-4 lines)'''\n",
    "    def forward(self, inputs, isidentity=False):\n",
    "      # YOUR CODE HERE\n",
    "      raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k43Tjtmlj1C2"
   },
   "source": [
    "## Question 1.5e: Let's test this behavior: (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "i8jUeaBBj4L1",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "080d544f7b5ca38aae1e9ec3efb6201f",
     "grade": true,
     "grade_id": "q1_5e",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test the IdentityModel\n",
    "model = LinearButSometimesIdentity(num_inputs=2, num_outputs=3)\n",
    "x_input = torch.tensor([[1, 2.]])\n",
    "\n",
    "out_with_linear = None\n",
    "out_with_identity = None\n",
    "\n",
    "'''TODO: pass the input into the model and call with and without the input identity option. (2 lines)'''\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(f\"input: {x_input}\")\n",
    "print(\"Network linear output: {}; network identity output: {}\".format(out_with_linear, out_with_identity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7Naz61HkOHU"
   },
   "source": [
    "## [ISU STUDENTS ONLY] Question 1.6: Debugging (3 points)\n",
    "\n",
    "What is the problem with the code below? Explain in writting one way to fix it (2-3 sentences; edits to the code won't be checked)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "id": "VkUveA41mD8K",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fa78c390089a11ade55bd51bc84e0956",
     "grade": true,
     "grade_id": "q1_6",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nGqc_YJ3kuNv"
   },
   "outputs": [],
   "source": [
    "class TwoLayerModel(nn.Module):\n",
    "    def __init__(self, num_inputs, num_hidden, num_outputs):\n",
    "        super(TwoLayerModel, self).__init__()\n",
    "        self.layer1 = LinearButSometimesIdentity(num_inputs, num_hidden)\n",
    "        self.layer2 = LinearButSometimesIdentity(num_hidden, num_outputs)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.layer1(inputs, isidentity=True)\n",
    "        x = self.activation(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "model = TwoLayerModel(2, 3, 1)\n",
    "x_input = torch.tensor([[1, 2.]])\n",
    "out = model(x_input)\n",
    "\n",
    "\n",
    "print(f\"input: {x_input}\")\n",
    "print(\"Network linear output: {}\".format(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6iUZNJbU6Nu"
   },
   "source": [
    "# Part II: Node Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gzsP50bF6Gb"
   },
   "source": [
    "\n",
    "\n",
    "In this Colab, we will write a full pipeline for **learning node embeddings**.\n",
    "We will go through the following 3 steps.\n",
    "\n",
    "To start, we will load a classic graph in network science, the [Karate Club Network](https://en.wikipedia.org/wiki/Zachary%27s_karate_club). We will explore multiple graph statistics for that graph.\n",
    "\n",
    "We will then work together to transform the graph structure into a PyTorch tensor, so that we can perform machine learning over the graph.\n",
    "\n",
    "Finally, we will finish the first learning algorithm on graphs: a node embedding model. For simplicity, our model here is simpler than DeepWalk / node2vec algorithms taught in the lecture. But it's still rewarding and challenging, as we will write it from scratch via PyTorch.\n",
    "\n",
    "Now let's get started! This Colab should take 1-2 hours to complete.\n",
    "\n",
    "**Note**: Make sure to **restart and run all** before submission, so that the intermediate variables / packages will carry over to the next cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FDkpByYYfSzb"
   },
   "source": [
    "## Setup\n",
    "We will heavily use NetworkX in this Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VWPkJjPAfVNW"
   },
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VqUnYT5qUZYh"
   },
   "source": [
    "## Zachary's karate club network\n",
    "\n",
    "The [Karate Club Network](https://en.wikipedia.org/wiki/Zachary%27s_karate_club) is a graph which describes a social network of 34 members of a karate club and documents links between members who interacted outside the club."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VIETqEfrfy5Y"
   },
   "outputs": [],
   "source": [
    "G = nx.karate_club_graph()\n",
    "\n",
    "# G is an undirected graph\n",
    "type(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hDvf3nm-ors4"
   },
   "outputs": [],
   "source": [
    "# Visualize the graph\n",
    "nx.draw(G, with_labels = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-MxvowibYl4x"
   },
   "source": [
    "## 2.1 Graph to Tensor\n",
    "We will then work together to transform the graph $G$ into a PyTorch tensor, so that we can perform machine learning over the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDA8PosrA-9V"
   },
   "source": [
    "### Setup\n",
    "Check if PyTorch is properly installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ntuPVat_BAf1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fko_2wSKYlun"
   },
   "source": [
    "### PyTorch tensor basics\n",
    "\n",
    "We can generate PyTorch tensor with all zeros, ones or random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W2ySw3m-A9qF"
   },
   "outputs": [],
   "source": [
    "# Generate 3 x 4 tensor with all ones\n",
    "ones = torch.ones(3, 4)\n",
    "print(ones)\n",
    "\n",
    "# Generate 3 x 4 tensor with all zeros\n",
    "zeros = torch.zeros(3, 4)\n",
    "print(zeros)\n",
    "\n",
    "# Generate 3 x 4 tensor with random values on the interval [0, 1)\n",
    "random_tensor = torch.rand(3, 4)\n",
    "print(random_tensor)\n",
    "\n",
    "# Get the shape of the tensor\n",
    "print(ones.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8mp66eHBxWC"
   },
   "source": [
    "PyTorch tensor contains elements for a single data type, the `dtype`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rQiOvKJJBwq4"
   },
   "outputs": [],
   "source": [
    "# Create a 3 x 4 tensor with all 32-bit floating point zeros\n",
    "zeros = torch.zeros(3, 4, dtype=torch.float32)\n",
    "print(zeros.dtype)\n",
    "\n",
    "# Change the tensor dtype to long (64-bit integer)\n",
    "zeros = zeros.type(torch.long)\n",
    "print(zeros.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9EfegIRDkk2"
   },
   "source": [
    "## Question 2.1a: Get the edge list of the karate club network and transform it into `torch.LongTensor` named `pos_edge_index` (for 'positive' edge list). What is the `torch.sum` value of `pos_edge_index` tensor? (2 points)\n",
    "\n",
    "Note: The sum in this case does not have interpretation. We ask this just to make grading easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "kEtVxMFID3ZT",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a9c503f6dd2afd1bb90310532501a78e",
     "grade": true,
     "grade_id": "q2_1a",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def graph_to_edge_list(G):\n",
    "  # TODO: Implement the function that returns the edge list of\n",
    "  # an nx.Graph. The returned edge_list should be a list of tuples\n",
    "  # where each tuple is a tuple representing an edge connected\n",
    "  # by two nodes (1-2 lines)\n",
    "\n",
    "  edge_list = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "  return edge_list\n",
    "\n",
    "def edge_list_to_tensor(edge_list):\n",
    "  # TODO: Implement the function that transforms the edge_list to\n",
    "  # tensor. The input edge_list is a list of tuples and the resulting\n",
    "  # tensor should have the shape [2, len(edge_list)] (1-2 lines).\n",
    "\n",
    "  # Note 1: You don't need to iteratively fill in the entries,\n",
    "  # you can overwrite the variable\n",
    "\n",
    "  edge_index = torch.tensor([])\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "  return edge_index\n",
    "\n",
    "pos_edge_list = graph_to_edge_list(G)\n",
    "pos_edge_index = edge_list_to_tensor(pos_edge_list)\n",
    "print(\"The pos_edge_index tensor has shape {}\".format(pos_edge_index.shape))\n",
    "print(\"The pos_edge_index tensor has sum value {}\".format(torch.sum(pos_edge_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UBL-ZmdHWqIu"
   },
   "source": [
    "## Question 2.1b: Please implement following function that samples negative edges. Then answer which edges (edge_1 to edge_5) are the negative edges in the karate club network? (8 points)\n",
    "\n",
    "\"Negative\" edges refer to the edges/links that do not exist in the graph. The term \"negative\" is borrowed from \"negative sampling\" in link prediction. It has nothing to do with the edge weights.\n",
    "\n",
    "For example, given an edge (src, dst), you should check that neither (src, dst) nor (dst, src) are edges in the Graph. If these hold true, then it is a negative edge.\n",
    "\n",
    "Hint: it is easier to use G than pos_edge_list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "4Ol4mWt-zr4u",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f9aea5e56b03bdeae27066815249051",
     "grade": true,
     "grade_id": "q2_1b",
     "locked": false,
     "points": 8,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(4099)\n",
    "\n",
    "# Which of following edges can be negative ones?\n",
    "edge_1 = (7, 1)\n",
    "edge_2 = (1, 33)\n",
    "edge_3 = (33, 22)\n",
    "edge_4 = (0, 4)\n",
    "edge_5 = (4, 2)\n",
    "\n",
    "def can_be_negative(G, edge):\n",
    "  # TODO: Implement the function that tests if an edge is a negative\n",
    "  # edge in graph G (2-4 lines).\n",
    "\n",
    "  is_negative = False\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "  return is_negative\n",
    "\n",
    "print(f\"Edge 1 can be a negative edge: {can_be_negative(G, edge_1)}\")\n",
    "print(f\"Edge 2 can be a negative edge: {can_be_negative(G, edge_2)}\")\n",
    "print(f\"Edge 3 can be a negative edge: {can_be_negative(G, edge_3)}\")\n",
    "print(f\"Edge 4 can be a negative edge: {can_be_negative(G, edge_4)}\")\n",
    "print(f\"Edge 5 can be a negative edge: {can_be_negative(G, edge_5)}\")\n",
    "\n",
    "def sample_negative_edges(G, num_neg_samples):\n",
    "  # TODO: Implement the function that returns a list of negative edges.\n",
    "  # The number of sampled negative edges is num_neg_samples. DIFFERENTLY than\n",
    "  # what we saw in class, we will not sample a fixed number of edges k for each\n",
    "  # node. Rather, you will sample uniformly from the set of non-edges. You don't\n",
    "  # need to consider the corner case when the number of possible negative edges\n",
    "  # is less than num_neg_samples. It should be ok as long as your implementation\n",
    "  # works on the karate club network. In this implementation, self loops should\n",
    "  # not be considered as either a positive or negative edge. Also, notice that\n",
    "  # the karate club network is an undirected graph, if (0, 1) is a positive\n",
    "  # edge, do you think (1, 0) can be a negative one? (6-10 lines)\n",
    "\n",
    "  neg_edge_list = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "  return neg_edge_list\n",
    "\n",
    "# Sample 78 negative edges\n",
    "neg_edge_list = sample_negative_edges(G, len(pos_edge_list))\n",
    "\n",
    "\n",
    "# Transform the negative edge list to tensor\n",
    "neg_edge_index = edge_list_to_tensor(neg_edge_list)\n",
    "print(\"The neg_edge_index tensor has shape {}\".format(neg_edge_index.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIRoqmnLonv_"
   },
   "source": [
    "## Question 2.2 [ISU STUDENTS ONLY]: Reimplement the function `sample_negative_edges` so that the node selection probability is proportional to its degree (for one of the end points). (7 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "_emnEqPitOIR",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "340dcd588db6f841eff706246868e000",
     "grade": true,
     "grade_id": "q2_2",
     "locked": false,
     "points": 7,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def sample_negative_edges_propto_degree(G, num_neg_samples):\n",
    "  # Q2.2 (7-10 lines)\n",
    "  neg_edge_list = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "  return neg_edge_list\n",
    "\n",
    "# Sample 78 negative edges\n",
    "neg_edge_list = sample_negative_edges_propto_degree(G, len(pos_edge_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wk9Q-a-9qGsw"
   },
   "source": [
    "## 2.3 Node Emebedding Learning\n",
    "\n",
    "Finally, we will finish the first learning algorithm on graphs: a node embedding model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDBxRQcZ_dUH"
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lnqn9H6s_ehX"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6gomAf8vxq0R"
   },
   "source": [
    "To write our own node embedding learning methods, we'll heavily use the [`nn.Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) module in PyTorch. Let's see how to use `nn.Embedding`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aRiWGuLAx5yx"
   },
   "outputs": [],
   "source": [
    "# Initialize an embedding layer\n",
    "# Suppose we want to have embedding for 4 items (e.g., nodes)\n",
    "# Each item is represented with 8 dimensional vector\n",
    "\n",
    "emb_sample = nn.Embedding(num_embeddings=4, embedding_dim=8)\n",
    "print('Sample embedding layer: {}'.format(emb_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bS9qQfeujEVh"
   },
   "source": [
    "We can select items from the embedding matrix, by using Tensor indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9AGIfP4QEDr8"
   },
   "outputs": [],
   "source": [
    "# Select an embedding in emb_sample\n",
    "id = torch.LongTensor([1])\n",
    "print(emb_sample(id))\n",
    "\n",
    "# Select multiple embeddings\n",
    "ids = torch.LongTensor([1, 3])\n",
    "print(emb_sample(ids))\n",
    "\n",
    "# Get the shape of the embedding weight matrix\n",
    "shape = emb_sample.weight.data.shape\n",
    "print(shape)\n",
    "\n",
    "# Overwrite the weight to tensor with all ones\n",
    "emb_sample.weight.data = torch.ones(shape)\n",
    "\n",
    "# Let's check if the emb is indeed initialized\n",
    "ids = torch.LongTensor([0, 3])\n",
    "print(emb_sample(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8MjBuDKaKIsM"
   },
   "source": [
    "## Question 2.3a: Now, it's your time to create node embedding matrix for the graph we have! (2 points)\n",
    "- We want to have **16 dimensional** vector for each node in the karate club network.\n",
    "- We want to initalize the matrix under **uniform distribution**, in the range of $[0, 1)$. We suggest you using [`torch.rand`](https://pytorch.org/docs/stable/generated/torch.rand.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "hMszSwRPKGn1",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e2697dc6224f58dce4fa3c329bffd1f1",
     "grade": true,
     "grade_id": "q2_3a",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Please do not change / reset the random seed\n",
    "torch.manual_seed(1)\n",
    "\n",
    "def create_node_emb(num_node=34, embedding_dim=16):\n",
    "  # TODO: Implement this function that will create the node embedding matrix.\n",
    "  # A torch.nn.Embedding layer will be returned. You do not need to change\n",
    "  # the values of num_node and embedding_dim. The weight matrix of returned\n",
    "  # layer should be initialized under uniform distribution (2 lines)\n",
    "\n",
    "  emb = None\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "  return emb\n",
    "\n",
    "emb = create_node_emb()\n",
    "ids = torch.LongTensor([0, 3])\n",
    "\n",
    "# Print the embedding layer\n",
    "print(\"Embedding: {}\".format(emb))\n",
    "\n",
    "# An example that gets the embeddings for node 0 and 3\n",
    "print(emb(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QfoANibTzyh"
   },
   "source": [
    "## Visualize the initial node embeddings\n",
    "One good way to understand an embedding matrix, is to visualize it in a 2D space.\n",
    "Here, we have implemented an embedding visualization function for you.\n",
    "We first do PCA to reduce the dimensionality of embeddings to a 2D space.\n",
    "Then we visualize each point, colored by the community it belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_LCoIkarhfYD"
   },
   "outputs": [],
   "source": [
    "def visualize_emb(emb):\n",
    "  X = emb.weight.data.numpy()\n",
    "  pca = PCA(n_components=2)\n",
    "  components = pca.fit_transform(X)\n",
    "  plt.figure(figsize=(6, 6))\n",
    "  club1_x = []\n",
    "  club1_y = []\n",
    "  club2_x = []\n",
    "  club2_y = []\n",
    "  for node in G.nodes(data=True):\n",
    "    if node[1]['club'] == 'Mr. Hi':\n",
    "      club1_x.append(components[node[0]][0])\n",
    "      club1_y.append(components[node[0]][1])\n",
    "    else:\n",
    "      club2_x.append(components[node[0]][0])\n",
    "      club2_y.append(components[node[0]][1])\n",
    "  plt.scatter(club1_x, club1_y, color=\"red\", label=\"Mr. Hi\")\n",
    "  plt.scatter(club2_x, club2_y, color=\"blue\", label=\"Officer\")\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "# Visualize the initial random embeddding\n",
    "visualize_emb(emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQIyuEz9ANb2"
   },
   "source": [
    "## Question 2.3b: Training the embedding! What is the best performance you can get? (6 points)\n",
    "\n",
    "We want to optimize our embeddings for the task of classifying edges as positive or negative. Given an edge and the embeddings for each node, the dot product of the embeddings, followed by a sigmoid, should give us the likelihood of that edge being either positive (output of sigmoid > 0.5) or negative (output of sigmoid < 0.5).\n",
    "\n",
    "Note that we're using the functions you wrote in the previous questions, _as well as the variables initialized in previous cells_. If you're running into issues, make sure your answers to questions 1-6 are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "RDeQTNNxqH0j",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "186c314ea616c7a01919b0d8d808d4a5",
     "grade": true,
     "grade_id": "q2_3b",
     "locked": false,
     "points": 6,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "import torch.nn as nn\n",
    "\n",
    "def accuracy(pred, label):\n",
    "  # TODO: Implement the accuracy function. This function takes the\n",
    "  # pred tensor (the resulting tensor after sigmoid) and the label\n",
    "  # tensor (torch.LongTensor). Predicted value greater than 0.5 will\n",
    "  # be classified as label 1. Else it will be classified as label 0.\n",
    "  # The returned accuracy should be rounded to 4 decimal places.\n",
    "  # For example, accuracy 0.82956 will be rounded to 0.8296. (3-4 lines)\n",
    "\n",
    "  accu = 0.0\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "  return accu\n",
    "\n",
    "def train(emb, loss_fn, train_label, train_edge):\n",
    "  # TODO: Train the embedding layer here. You can also change epochs and\n",
    "  # learning rate. In general, you need to implement (12+ lines):\n",
    "  # (1) Get the embeddings of the nodes in train_edge\n",
    "  # (2) Dot product the embeddings between each node pair\n",
    "  # (3) Feed the dot product result into sigmoid\n",
    "  # (4) Feed the sigmoid output into the loss_fn\n",
    "  # (5) Print both loss and accuracy of each epoch\n",
    "  # (6) Update the embeddings using the loss and optimizer\n",
    "  # (as a sanity check, the loss should decrease during training)\n",
    "\n",
    "  epochs = 500\n",
    "  learning_rate = 0.1\n",
    "\n",
    "  optimizer = SGD(emb.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "  for i in range(epochs):\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "  return emb\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "print(pos_edge_index.shape)\n",
    "\n",
    "# Generate the positive and negative labels\n",
    "pos_label = torch.ones(pos_edge_index.shape[1], )\n",
    "neg_label = torch.zeros(neg_edge_index.shape[1], )\n",
    "\n",
    "# Concat positive and negative labels into one tensor\n",
    "train_label = torch.cat([pos_label, neg_label], dim=0)\n",
    "\n",
    "# Concat positive and negative edges into one tensor\n",
    "# Since the network is very small, we do not split the edges into val/test sets\n",
    "train_edge = torch.cat([pos_edge_index, neg_edge_index], dim=1)\n",
    "print(train_edge.shape)\n",
    "\n",
    "train(emb, loss_fn, train_label, train_edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4bh0Mz_nkPtA"
   },
   "source": [
    "## 2.4. Interpreting the training progress\n",
    "\n",
    "In the previous question, you should see the accuracy should be 1.0 around epoch 150. Yet, the loss will continue to decrease after that. Based on this observation answer the questions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQf3gu8LlLi1"
   },
   "source": [
    "## Question 2.4a: Why does the loss continues to decrease after the accuracy converges to 1.0? (1-3 sentences; 2 points)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "id": "R4Q7i2qglfwV",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c8be8967545e0cdfc26710f5e0248d1c",
     "grade": true,
     "grade_id": "q2_4a",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gNLRJs6emEGR"
   },
   "source": [
    "## Question 2.4b: Typically, an accuracy of 1.0 in training indicates overfitting. Why is this not a problem here? Think about the type of training or our goal in obtaining these embeddings. (1-2 sentences; 1 point)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "id": "W4mV8N7VmkeL",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c5d1549ec75dfddf3679b026cdc35a04",
     "grade": true,
     "grade_id": "q2_4b",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WX2PSXnTDiNi"
   },
   "source": [
    "## Visualize the final node embeddings\n",
    "Visualize your final embedding here!\n",
    "You can visually compare the figure with the previous embedding figure.\n",
    "After training, you should oberserve that the two classes are more evidently separated.\n",
    "This is a great sanitity check for your implementation as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MtNgl4VhYKow"
   },
   "outputs": [],
   "source": [
    "# Visualize the final learned embedding\n",
    "visualize_emb(emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjYzwI76AyIx"
   },
   "source": [
    "# Part III: GCN and PyTorch Geometric\n",
    "\n",
    "In Part III, we will work to construct our own graph neural network using PyTorch Geometric (PyG) and then apply that model on two Open Graph Benchmark (OGB) datasets. These two datasets will be used to benchmark your model's performance on one graph-based tasks: node property prediction, predicting properties of single nodes.\n",
    "\n",
    "First, we will learn how PyTorch Geometric stores graphs as PyTorch tensors.\n",
    "\n",
    "Then, we will load and inspect one of the Open Graph Benchmark (OGB) datasets by using the `ogb` package. OGB is a collection of realistic, large-scale, and diverse benchmark datasets for machine learning on graphs. The `ogb` package not only provides data loaders for each dataset but also model evaluators.\n",
    "\n",
    "Lastly, we will build our own graph neural network using PyTorch Geometric. We will then train and evaluate our model on the OGB node property prediction and graph property prediction tasks.\n",
    "\n",
    "**Note**: Make sure to **sequentially run all the cells in each section**, so that the intermediate variables / packages will carry over to the next cell\n",
    "\n",
    "We recommend you save a copy of this colab in your drive so you don't lose progress!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hXeaBn5Pdsd"
   },
   "source": [
    "## Device\n",
    "You might need to use a GPU for this Colab to run quickly. HOWEVER, you should start with a CPU node and make sure everything is working before you switch.\n",
    "\n",
    "Please click `Runtime` and then `Change runtime type`. Then set the `hardware accelerator` to **GPU**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KcWrgnf-PtOh"
   },
   "source": [
    "## Setup\n",
    "The installation of PyG on Colab can be a little bit tricky. First let us check which version of PyTorch you are running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nSbFgM9oPv1H"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "print(\"PyTorch has version {}\".format(torch.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RqDINsvFPx40"
   },
   "outputs": [],
   "source": [
    "!pip install torch==2.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpyj3MsNP1Q5"
   },
   "source": [
    "Download the necessary packages for PyG. Make sure that your version of torch matches the output from the cell above. In case of any issues, more information can be found on the [PyG's installation page](https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D55hRXLgP7Xx"
   },
   "outputs": [],
   "source": [
    "# Install torch geometric\n",
    "torch_version = str(torch.__version__)\n",
    "scatter_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
    "sparse_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
    "!pip install torch-scatter -f $scatter_src\n",
    "!pip install torch-sparse -f $sparse_src\n",
    "!pip install torch-geometric\n",
    "!pip install ogb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0z7L72HQALZ"
   },
   "source": [
    "## 3.1 PyTorch Geometric (Datasets and Data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0K4nNi_pQFsg"
   },
   "source": [
    "PyTorch Geometric has two classes for storing and/or transforming graphs into tensor format. One is `torch_geometric.datasets`, which contains a variety of common graph datasets. Another is `torch_geometric.data`, which provides the data handling of graphs in PyTorch tensors.\n",
    "\n",
    "In this section, we will learn how to use `torch_geometric.datasets` and `torch_geometric.data` together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cau69UimQJR4"
   },
   "source": [
    "## PyG Datasets\n",
    "\n",
    "The `torch_geometric.datasets` class has many common graph datasets. Here we will explore its usage through one example dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AQDtNFQsQLwE"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "\n",
    "root = './enzymes'\n",
    "name = 'ENZYMES'\n",
    "\n",
    "# The ENZYMES dataset\n",
    "pyg_dataset= TUDataset(root, name)\n",
    "\n",
    "# You will find that there are 600 graphs in this dataset\n",
    "print(pyg_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZ7GoJSxQQF0"
   },
   "source": [
    "## Question 3.1a: What is the number of classes and number of features in the ENZYMES dataset? (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "VAzAwqyQQS5S",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "245ca70171cec761895c5fea2eb4fe13",
     "grade": true,
     "grade_id": "q3_1a",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_num_classes(pyg_dataset):\n",
    "  # TODO: Implement a function that takes a PyG dataset object\n",
    "  # and returns the number of classes for that dataset.\n",
    "\n",
    "  num_classes = 0\n",
    "\n",
    "  ############# Your code here ############\n",
    "  ## (~1 line of code)\n",
    "  ## Note\n",
    "  ## 1. Colab autocomplete functionality might be useful.\n",
    "  # YOUR CODE HERE\n",
    "  raise NotImplementedError()\n",
    "  #########################################\n",
    "\n",
    "  return num_classes\n",
    "\n",
    "def get_num_features(pyg_dataset):\n",
    "  # TODO: Implement a function that takes a PyG dataset object\n",
    "  # and returns the number of features for that dataset.\n",
    "\n",
    "  num_features = 0\n",
    "\n",
    "  ############# Your code here ############\n",
    "  ## (~1 line of code)\n",
    "  ## Note\n",
    "  ## 1. Colab autocomplete functionality might be useful.\n",
    "  # YOUR CODE HERE\n",
    "  raise NotImplementedError()\n",
    "  #########################################\n",
    "\n",
    "  return num_features\n",
    "\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "  num_classes = get_num_classes(pyg_dataset)\n",
    "  num_features = get_num_features(pyg_dataset)\n",
    "  print(\"{} dataset has {} classes\".format(name, num_classes))\n",
    "  print(\"{} dataset has {} features\".format(name, num_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjPW-cVQQcyw"
   },
   "source": [
    "## PyG Data\n",
    "\n",
    "Each PyG dataset stores a list of `torch_geometric.data.Data` objects, where each `torch_geometric.data.Data` object represents a graph. We can easily get the `Data` object by indexing into the dataset.\n",
    "\n",
    "For more information such as what is stored in the `Data` object, please refer to the [documentation](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html#torch_geometric.data.Data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvIc1hrqQid-"
   },
   "source": [
    "## Question 3.1b: What is the label of the graph with index 100 in the ENZYMES dataset? (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "u3Ncf4YZQkLF",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3c3653d899b77de5b9599c0ccc12d1ee",
     "grade": true,
     "grade_id": "q3_1b",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tqvhxp1YQoVT"
   },
   "source": [
    "## Question 3.1c: How many edges does the graph with index 200 have? (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "FaNHkFIgQv6G",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9c38368fbca5f62542f4f0530f226791",
     "grade": true,
     "grade_id": "q3_1c",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_graph_num_edges(pyg_dataset, idx):\n",
    "  # TODO: Implement a function that takes a PyG dataset object,\n",
    "  # the index of a graph in the dataset, and returns the number of\n",
    "  # edges in the graph (as an integer). You should not count an edge\n",
    "  # twice if the graph is undirected. For example, in an undirected\n",
    "  # graph G, if two nodes v and u are connected by an edge, this edge\n",
    "  # should only be counted once.\n",
    "\n",
    "  num_edges = 0\n",
    "\n",
    "  ############# Your code here ############\n",
    "  ## Note:\n",
    "  ## 1. You can't return the data.num_edges directly\n",
    "  ## 2. We assume the graph is undirected\n",
    "  ## 3. Look at the PyG dataset built in functions\n",
    "  ## (~3-4 lines of code)\n",
    "  # YOUR CODE HERE\n",
    "  raise NotImplementedError()\n",
    "  #########################################\n",
    "\n",
    "  return num_edges\n",
    "\n",
    "idx = 200\n",
    "num_edges = get_graph_num_edges(pyg_dataset, idx)\n",
    "print('Graph with index {} has {} edges'.format(idx, num_edges))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X30uDdyNQ4db"
   },
   "source": [
    "## 3.2 Open Graph Benchmark (OGB)\n",
    "\n",
    "The Open Graph Benchmark (OGB) is a collection of realistic, large-scale, and diverse benchmark datasets for machine learning on graphs. Its datasets are automatically downloaded, processed, and split using the OGB Data Loader. The model performance can then be evaluated by using the OGB Evaluator in a unified manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5v0FdSCqRGBr"
   },
   "source": [
    "### Dataset and Data\n",
    "\n",
    "OGB also supports PyG dataset and data classes. Here we take a look on the `ogbn-arxiv` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sm79TsRvRFGX"
   },
   "outputs": [],
   "source": [
    "import torch_geometric.transforms as T\n",
    "from ogb.nodeproppred import PygNodePropPredDataset\n",
    "\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "  dataset_name = 'ogbn-arxiv'\n",
    "  # Load the dataset and transform it to sparse tensor\n",
    "  dataset = PygNodePropPredDataset(name=dataset_name,\n",
    "                                  transform=T.ToSparseTensor())\n",
    "  print('The {} dataset has {} graph'.format(dataset_name, len(dataset)))\n",
    "\n",
    "  # Extract the graph\n",
    "  data = dataset[0]\n",
    "  print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3FPWa-kzRLnQ"
   },
   "source": [
    "## Question 3.2: How many features are in the ogbn-arxiv graph? (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "SCqCSeNXRUcI",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fb24a839605c53253aca0e29f4e0e3de",
     "grade": true,
     "grade_id": "q3_2",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def graph_num_features(data):\n",
    "  # TODO: Implement a function that takes a PyG data object,\n",
    "  # and returns the number of features in the graph (as an integer).\n",
    "\n",
    "  num_features = 0\n",
    "\n",
    "  ############# Your code here ############\n",
    "  ## (~1 line of code)\n",
    "  # YOUR CODE HERE\n",
    "  raise NotImplementedError()\n",
    "  #########################################\n",
    "\n",
    "  return num_features\n",
    "\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "  num_features = graph_num_features(data)\n",
    "  print('The graph has {} features'.format(num_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dufp-swsRe8_"
   },
   "source": [
    "## 3.3 GNN: Node Property Prediction\n",
    "\n",
    "In this section we will build our first graph neural network using PyTorch Geometric. Then we will apply it to the task of node property prediction (node classification).\n",
    "\n",
    "Specifically, we will use GCN as the foundation for your graph neural network ([Kipf et al. (2017)](https://arxiv.org/pdf/1609.02907.pdf)). To do so, we will work with PyG's built-in `GCNConv` layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJcQyvHfRTVP"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wSDGnZ9IRlrQ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "print(torch.__version__)\n",
    "\n",
    "# The PyG built-in GCNConv\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUDHWMDjRrOB"
   },
   "source": [
    "## Load and Preprocess the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lDbFOtoxRr6k"
   },
   "outputs": [],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "  dataset_name = 'ogbn-arxiv'\n",
    "  dataset = PygNodePropPredDataset(name=dataset_name,\n",
    "                                  transform=T.Compose(\n",
    "                                      [T.ToUndirected(),\n",
    "                                       T.ToSparseTensor()]))\n",
    "  data = dataset[0]\n",
    "\n",
    "  # # Make the adjacency matrix to symmetric\n",
    "  # data.adj_t = data.adj_t.to_symmetric()\n",
    "\n",
    "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "  # If you use GPU, the device should be cuda\n",
    "  print('Device: {}'.format(device))\n",
    "\n",
    "  data = data.to(device)\n",
    "  split_idx = dataset.get_idx_split()\n",
    "  train_idx = split_idx['train'].to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pFiTpkhfR05E"
   },
   "source": [
    "## Question 3.3a: GCN Model (7 points)\n",
    "\n",
    "Now we will implement our GCN model!\n",
    "\n",
    "Please follow the figure below to implement the `forward` function.\n",
    "\n",
    "\n",
    "![test](https://drive.google.com/uc?id=128AuYAXNXGg7PIhJJ7e420DoPWKb-RtL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "RmQ1U90iR3TI",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "59fc219e85e9960186683a4a79ca358c",
     "grade": true,
     "grade_id": "q3_3aa",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers,\n",
    "                 dropout, return_embeds=False):\n",
    "        # TODO: Implement a function that initializes self.convs,\n",
    "        # self.bns, and self.softmax.\n",
    "\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        # A list of GCNConv layers\n",
    "        self.convs = None\n",
    "\n",
    "        # A list of 1D batch normalization layers\n",
    "        self.bns = None\n",
    "\n",
    "        # The log softmax layer\n",
    "        self.softmax = None\n",
    "\n",
    "        ############# Your code here ############\n",
    "        ## Note:\n",
    "        ## 1. You should use torch.nn.ModuleList for self.convs and self.bns\n",
    "        ## 2. self.convs has num_layers GCNConv layers\n",
    "        ## 3. self.bns has num_layers - 1 BatchNorm1d layers\n",
    "        ## 4. You should use torch.nn.LogSoftmax for self.softmax\n",
    "        ## 5. The parameters you can set for GCNConv include 'in_channels' and\n",
    "        ## 'out_channels'. For more information please refer to the documentation:\n",
    "        ## https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GCNConv\n",
    "        ## 6. The only parameter you need to set for BatchNorm1d is 'num_features'\n",
    "        ## For more information please refer to the documentation:\n",
    "        ## https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html\n",
    "        ## (~10 lines of code)\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        #########################################\n",
    "\n",
    "        # Probability of an element getting zeroed\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Skip classification layer and return node embeddings\n",
    "        self.return_embeds = return_embeds\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "\n",
    "    def forward(self, adj_t, x):\n",
    "        # TODO: Implement a function that takes the feature tensor x and\n",
    "        # edge_index tensor adj_t and returns the output tensor as\n",
    "        # shown in the figure.\n",
    "\n",
    "        out = None\n",
    "\n",
    "        ############# Your code here ############\n",
    "        ## Note:\n",
    "        ## 1. Construct the network as shown in the figure\n",
    "        ## 2. torch.nn.functional.relu and torch.nn.functional.dropout are useful\n",
    "        ## For more information please refer to the documentation:\n",
    "        ## https://pytorch.org/docs/stable/nn.functional.html\n",
    "        ## 3. Don't forget to set F.dropout training to self.training\n",
    "        ## 4. If return_embeds is True, then skip the last softmax layer\n",
    "        ## (~8 lines of code; Colab auto-complete will make 1 mistake)\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        #########################################\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "E-GQFfwtR5q6",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e6bd0ef9767f0bca4f0a1e9c51e792b7",
     "grade": true,
     "grade_id": "q3_3ab",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train(model, data, train_idx, optimizer, loss_fn):\n",
    "    # TODO: Implement a function that trains the model by\n",
    "    # using the given optimizer and loss_fn.\n",
    "    model.train()\n",
    "    loss = 0\n",
    "\n",
    "    ############# Your code here ############\n",
    "    ## Note:\n",
    "    ## 1. Zero grad the optimizer\n",
    "    ## 2. Feed the data into the model\n",
    "    ## 3. Slice the model output and label by train_idx\n",
    "    ## 4. Feed the sliced output and label to loss_fn\n",
    "    ## (~3-5 lines of code)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    #########################################\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "XHYve7TAR7SG",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ce3a77e587d6486cb4339ae75aadbfaf",
     "grade": true,
     "grade_id": "q3_3ac",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test function here\n",
    "@torch.no_grad()\n",
    "def test(model, data, split_idx, evaluator, save_model_results=False):\n",
    "    # TODO: Implement a function that tests the model by\n",
    "    # using the given split_idx and evaluator.\n",
    "    model.eval()\n",
    "\n",
    "    # The output of model on all data\n",
    "    out = None\n",
    "\n",
    "    ############# Your code here ############\n",
    "    ## (~1 line of code)\n",
    "    ## Note:\n",
    "    ## 1. No index slicing here\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    #########################################\n",
    "\n",
    "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "    train_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['train']],\n",
    "        'y_pred': y_pred[split_idx['train']],\n",
    "    })['acc']\n",
    "    valid_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['valid']],\n",
    "        'y_pred': y_pred[split_idx['valid']],\n",
    "    })['acc']\n",
    "    test_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['test']],\n",
    "        'y_pred': y_pred[split_idx['test']],\n",
    "    })['acc']\n",
    "\n",
    "    if save_model_results:\n",
    "      print (\"Saving Model Predictions\")\n",
    "\n",
    "      data = {}\n",
    "      data['y_pred'] = y_pred.view(-1).cpu().detach().numpy()\n",
    "\n",
    "      df = pd.DataFrame(data=data)\n",
    "      # Save locally as csv\n",
    "      df.to_csv('ogbn-arxiv_node.csv', sep=',', index=False)\n",
    "\n",
    "\n",
    "    return train_acc, valid_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tav11h7QR9hg"
   },
   "outputs": [],
   "source": [
    "# Please do not change the args\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "  args = {\n",
    "      'device': device,\n",
    "      'num_layers': 3,\n",
    "      'hidden_dim': 256,\n",
    "      'dropout': 0.5,\n",
    "      'lr': 0.01,\n",
    "      'epochs': 100,\n",
    "  }\n",
    "  args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gxlwCsdpR-0K"
   },
   "outputs": [],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "  model = GCN(data.num_features, args['hidden_dim'],\n",
    "              dataset.num_classes, args['num_layers'],\n",
    "              args['dropout']).to(device)\n",
    "  evaluator = Evaluator(name='ogbn-arxiv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XGgFRFAKSAqv"
   },
   "outputs": [],
   "source": [
    "# Please do not change these args\n",
    "# Training should take <10min using GPU runtime\n",
    "import copy\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "  # reset the parameters to initial random value\n",
    "  model.reset_parameters()\n",
    "\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
    "  loss_fn = F.nll_loss\n",
    "\n",
    "  best_model = None\n",
    "  best_valid_acc = 0\n",
    "\n",
    "  for epoch in range(1, 1 + args[\"epochs\"]):\n",
    "    loss = train(model, data, train_idx, optimizer, loss_fn)\n",
    "    result = test(model, data, split_idx, evaluator)\n",
    "    train_acc, valid_acc, test_acc = result\n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        best_model = copy.deepcopy(model)\n",
    "    print(f'Epoch: {epoch:02d}, '\n",
    "          f'Loss: {loss:.4f}, '\n",
    "          f'Train: {100 * train_acc:.2f}%, '\n",
    "          f'Valid: {100 * valid_acc:.2f}% '\n",
    "          f'Test: {100 * test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "asRwyh9LSsQD"
   },
   "source": [
    "## Question 3.3b: What are your `best_model` validation and test accuracies? (5 extra points)\n",
    "\n",
    "Run the cell below to see the results of your best of model and save your model's predictions to a file named *ogbn-arxiv_node.csv*. You can view this file by clicking on the *Folder* icon on the left side pannel.\n",
    "\n",
    "You can get up to 5 extra points for each percentual point of accuracy above 70% on the test set. For example: if you get 70.8%, this will be rounded to 71%, and you will get 1 extra point.\n",
    "\n",
    "It is ALLOWED to change the hypers and the network architecture, as long as you are only using GCN, BatchNormalization and Dropout layers/ If you change any of these, write the corresponding code in a new cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "nRnTCVX3TcwF",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e7726a15d17f4ed13ecfea84952b8f90",
     "grade": true,
     "grade_id": "q3_3b",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1I2Tfm1SZhB"
   },
   "source": [
    "## Question 3.3c [ISU ONLY] : Reimplement the training loop with Early Stopping with a `patience` parameter (6 points)\n",
    "\n",
    "In slide 67 of this [lecture](https://canvas.wpi.edu/courses/70386/files/folder/slides?preview=7291159) we covered the Early Stopping algorithm.\n",
    "\n",
    "Instead of just stopping the training immediately after the validation loss goes up, use a variable `patience` to set the number of consecutive rounds in which the loss must increase before the training is stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "deletable": false,
    "id": "ItH1Sm_VSiD1",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a56f31fa66eba6839d4f378feed11be",
     "grade": true,
     "grade_id": "q3_3c",
     "locked": false,
     "points": 6,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "cd262f94-7b19-4785-8be9-8df6fde6a344"
   },
   "outputs": [],
   "source": [
    "# Please do not change these args\n",
    "# Training should take <10min using GPU runtime\n",
    "import copy\n",
    "\n",
    "patience = 3\n",
    "\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "  # reset the parameters to initial random value\n",
    "  model.reset_parameters()\n",
    "\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
    "  loss_fn = F.nll_loss\n",
    "\n",
    "  best_model = None\n",
    "  best_valid_acc = 0\n",
    "\n",
    "  # TODO: start by copying the for loop from previous cell\n",
    "  # Implement Early Stopping with patience (~6 new lines)\n",
    "  # YOUR CODE HERE\n",
    "  raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTNyrAoSVeq9"
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_E7J_GkVhY_"
   },
   "source": [
    "When you submit your assignment, you will have to download this file as an `.ipynb` file. Please name this file `CS4099_Colab_3.ipynb`."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
